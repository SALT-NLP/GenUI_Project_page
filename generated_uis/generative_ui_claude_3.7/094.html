<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Reinforcement Learning Explained</title>
    <style>
        :root {
            --primary: #2563eb;
            --primary-light: #3b82f6;
            --primary-dark: #1d4ed8;
            --secondary: #10b981;
            --text: #111827;
            --text-light: #4b5563;
            --background: #ffffff;
            --background-alt: #f3f4f6;
            --border: #e5e7eb;
            --shadow: rgba(0, 0, 0, 0.1);
            --card-bg: #f9fafb;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Roboto, -apple-system, BlinkMacSystemFont, sans-serif;
            line-height: 1.6;
            color: var(--text);
            background-color: var(--background-alt);
            padding: 0;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 2rem;
            display: grid;
            grid-template-columns: 1fr;
            gap: 2rem;
        }

        @media (min-width: 768px) {
            .container {
                grid-template-columns: 260px 1fr;
            }
        }

        header {
            background-color: var(--primary);
            color: white;
            padding: 1.5rem 2rem;
            box-shadow: 0 4px 6px var(--shadow);
            position: sticky;
            top: 0;
            z-index: 100;
        }

        h1 {
            font-size: 2rem;
            margin-bottom: 0.5rem;
        }

        header p {
            opacity: 0.9;
        }

        .hidden {
            display: none;
        }

        .card {
            background-color: var(--card-bg);
            border-radius: 8px;
            box-shadow: 0 2px 4px var(--shadow);
            overflow: hidden;
            transition: transform 0.3s ease, box-shadow 0.3s ease;
            border: 1px solid var(--border);
        }

        .card:hover {
            transform: translateY(-5px);
            box-shadow: 0 6px 12px var(--shadow);
        }

        .card-header {
            padding: 1.25rem;
            background-color: var(--primary);
        }

        .card-header h3,
        .card-header h2 {
            color: white;
        }

        .card-body {
            padding: 1.5rem;
            color: var(--text);
        }

        h2 {
            font-size: 1.5rem;
            margin-bottom: 1rem;
            color: var(--primary);
        }

        h3 {
            font-size: 1.25rem;
            margin: 1.5rem 0 0.75rem;
            color: var(--primary-dark);
        }

        p,
        ul,
        ol {
            margin-bottom: 1rem;
        }

        ul,
        ol {
            padding-left: 1.5rem;
        }

        .sidebar {
            display: flex;
            flex-direction: column;
            gap: 1rem;
            position: sticky;
            top: 12rem;
            height: calc(100vh - 14rem);
            overflow-y: auto;
            background-color: transparent;
            padding: 0 0.5rem 0 0;
        }

        .search-container {
            position: sticky;
            top: 0;
            z-index: 10;
            margin-bottom: 1rem;
            width: 100%;
            background-color: var(--background-alt);
            padding: 1rem 0.5rem 0.5rem 0;
            border-bottom: 1px solid var(--border);
        }

        .sidebar-nav {
            background-color: var(--background);
            border-radius: 8px;
            box-shadow: 0 2px 4px var(--shadow);
            overflow: hidden;
            margin-bottom: 0;
        }

        .sidebar-nav-header {
            background-color: var(--primary);
            color: white;
            padding: 1rem;
            font-weight: bold;
        }

        .sidebar-nav ul {
            list-style: none;
            padding: 0;
            margin: 0;
        }

        .sidebar-nav li {
            border-bottom: 1px solid var(--border);
        }

        .sidebar-nav li:last-child {
            border-bottom: none;
        }

        .sidebar-nav a {
            display: block;
            padding: 0.75rem 1rem;
            color: var(--text);
            text-decoration: none;
            transition: background-color 0.2s ease, color 0.2s ease;
        }

        .sidebar-nav a:hover,
        .sidebar-nav a.active {
            background-color: var(--background-alt);
            color: var(--primary);
        }

        .sidebar-nav a:focus {
            outline: 2px solid var(--primary-light);
            outline-offset: -2px;
        }

        .main-content {
            display: flex;
            flex-direction: column;
            gap: 2rem;
        }

        .btn {
            display: inline-block;
            background-color: var(--primary);
            color: white;
            padding: 0.5rem 1rem;
            border-radius: 4px;
            text-decoration: none;
            border: none;
            cursor: pointer;
            font-weight: 500;
            transition: background-color 0.2s ease, transform 0.1s ease;
        }

        .btn:hover {
            background-color: var(--primary-dark);
        }

        .btn:active {
            transform: translateY(1px);
        }

        .btn:focus {
            outline: 2px solid var(--primary-light);
            outline-offset: 2px;
        }

        .btn-secondary {
            background-color: var(--secondary);
        }

        .btn-secondary:hover {
            background-color: #0da271;
        }

        .btn:disabled {
            background-color: var(--text-light);
            cursor: not-allowed;
        }

        .algorithm-comparison {
            display: grid;
            grid-template-columns: 1fr;
            gap: 1rem;
        }

        @media (min-width: 768px) {
            .algorithm-comparison {
                grid-template-columns: repeat(3, 1fr);
            }
        }

        .badge {
            display: inline-block;
            background-color: var(--primary-light);
            color: white;
            padding: 0.25rem 0.5rem;
            border-radius: 999px;
            font-size: 0.75rem;
            margin-right: 0.5rem;
            margin-bottom: 0.5rem;
        }

        .rl-diagram {
            width: 100%;
            max-width: 600px;
            margin: 2rem auto;
            display: block;
            border-radius: 12px;
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.08), 0 2px 5px rgba(0, 0, 0, 0.05);
            border: 1px solid var(--border);
            background-color: var(--background);
            padding: 1.5rem;
            transition: box-shadow 0.3s ease, transform 0.3s ease;
        }

        .rl-diagram:hover {
            transform: translateY(-3px);
            box-shadow: 0 8px 20px rgba(0, 0, 0, 0.1), 0 4px 8px rgba(0, 0, 0, 0.07);
        }

        .rl-diagram text {
            font-family: inherit;
        }

        .rl-diagram path {
            stroke-linecap: round;
            stroke-linejoin: round;
        }

        .interactive-demo {
            margin-top: 1.5rem;
            border: 1px solid var(--border);
            border-radius: 8px;
            padding: 1rem;
        }

        .demo-controls {
            display: flex;
            gap: 1rem;
            margin-top: 1rem;
            flex-wrap: wrap;
        }

        .demo-grid {
            display: grid;
            grid-template-columns: repeat(5, 1fr);
            gap: 0.25rem;
            margin-top: 1rem;
            max-width: 300px;
            margin-left: auto;
            margin-right: auto;
        }

        .grid-cell {
            aspect-ratio: 1;
            background-color: var(--background-alt);
            border-radius: 4px;
            display: flex;
            align-items: center;
            justify-content: center;
            font-weight: bold;
            position: relative;
            transition: background-color 0.3s ease, transform 0.2s ease;
            font-size: 0.8rem;
        }

        .grid-cell:hover {
            transform: scale(1.05);
        }

        .grid-cell.goal {
            background-color: var(--secondary);
            color: white;
        }

        .grid-cell.obstacle {
            background-color: var(--text);
            color: white;
        }

        .grid-cell.agent {
            color: var(--primary-dark);
        }

        .grid-cell.agent::before {
            content: "ü§ñ";
            font-size: 1.2rem;
        }

        .progress-container {
            width: 100%;
            background-color: var(--background-alt);
            border-radius: 999px;
            margin: 1rem 0;
            height: 0.5rem;
            overflow: hidden;
        }

        .progress-bar {
            height: 100%;
            background-color: var(--primary);
            width: 0%;
            transition: width 0.3s ease;
        }

        .info-box {
            background-color: rgba(37, 99, 235, 0.1);
            border-left: 4px solid var(--primary);
            padding: 1rem;
            margin: 1.5rem 0;
            border-radius: 0 4px 4px 0;
        }

        .tabs {
            display: flex;
            border-bottom: 1px solid var(--border);
            margin-bottom: 1rem;
            overflow-x: auto;
            -webkit-overflow-scrolling: touch;
        }

        .tab {
            padding: 0.75rem 1.25rem;
            cursor: pointer;
            border-bottom: 2px solid transparent;
            transition: all 0.2s ease;
            white-space: nowrap;
        }

        .tab:hover {
            color: var(--primary);
            background-color: rgba(37, 99, 235, 0.05);
        }

        .tab.active {
            border-bottom-color: var(--primary);
            color: var(--primary);
            font-weight: 500;
        }

        .tab:focus {
            outline: none;
            box-shadow: 0 0 0 2px var(--primary-light);
        }

        .tab-content {
            display: none;
            animation: fadeIn 0.3s ease-in-out;
        }

        .tab-content.active {
            display: block;
        }

        @keyframes fadeIn {
            from {
                opacity: 0;
                transform: translateY(10px);
            }

            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        /* Dark mode toggle */
        .theme-toggle {
            position: absolute;
            top: 1.5rem;
            right: 2rem;
            background: transparent;
            border: none;
            color: white;
            cursor: pointer;
            display: flex;
            align-items: center;
            font-size: 0.875rem;
        }

        .theme-toggle svg {
            margin-right: 0.5rem;
            width: 20px;
            height: 20px;
        }

        /* Dark mode styles */
        .dark-mode {
            --primary: #3b82f6;
            --primary-light: #60a5fa;
            --primary-dark: #2563eb;
            --secondary: #10b981;
            --text: #f3f4f6;
            --text-light: #d1d5db;
            --background: #1f2937;
            --background-alt: #111827;
            --border: #374151;
            --shadow: rgba(0, 0, 0, 0.3);
            --card-bg: #1a222e;
        }

        /* High Contrast Mode Styles */
        .high-contrast {
            --primary: #0000ff;
            /* Bright blue */
            --primary-light: #3333ff;
            --primary-dark: #0000cc;
            --secondary: #00ff00;
            /* Bright green */
            --text: #000000;
            /* Black */
            --text-light: #333333;
            --background: #ffffff;
            /* White */
            --background-alt: #eeeeee;
            --border: #000000;
            /* Black */
            --shadow: rgba(0, 0, 0, 0.5);
            --card-bg: #ffffff;
        }

        .high-contrast.dark-mode {
            --primary: #ffff00;
            /* Bright yellow */
            --primary-light: #ffff33;
            --primary-dark: #cccc00;
            --secondary: #00ffff;
            /* Bright cyan */
            --text: #ffffff;
            /* White */
            --text-light: #cccccc;
            --background: #000000;
            /* Black */
            --background-alt: #1a1a1a;
            --border: #ffffff;
            /* White */
            --card-bg: #000000;
        }

        .high-contrast .card,
        .high-contrast .sidebar-nav,
        .high-contrast .search-input,
        .high-contrast .comparison-table th,
        .high-contrast .comparison-table td {
            border: 2px solid var(--border);
        }

        .high-contrast .btn {
            border: 2px solid var(--border);
        }

        .high-contrast .glossary-term {
            border-bottom: 2px dotted var(--primary-dark);
        }

        .high-contrast .search-highlight {
            background-color: var(--primary) !important;
            color: var(--background) !important;
            font-weight: bold;
        }

        /* Reduced Motion Styles */
        .reduced-motion *,
        .reduced-motion *::before,
        .reduced-motion *::after {
            animation-duration: 0.01ms !important;
            animation-iteration-count: 1 !important;
            transition-duration: 0.01ms !important;
            transition-delay: 0ms !important;
            scroll-behavior: auto !important;
        }

        /* Tooltip styles */
        .tooltip {
            position: relative;
            display: inline-block;
        }

        .tooltip .tooltiptext {
            visibility: hidden;
            width: 200px;
            background-color: var(--text);
            color: var(--background);
            text-align: center;
            border-radius: 6px;
            padding: 8px;
            position: absolute;
            z-index: 1;
            bottom: 125%;
            left: 50%;
            transform: translateX(-50%);
            opacity: 0;
            transition: opacity 0.3s;
            box-shadow: 0 2px 8px var(--shadow);
            font-size: 0.875rem;
        }

        .tooltip:hover .tooltiptext {
            visibility: visible;
            opacity: 1;
        }

        /* Animation for the agent */
        @keyframes pulse {
            0% {
                transform: scale(1);
            }

            50% {
                transform: scale(1.1);
            }

            100% {
                transform: scale(1);
            }
        }

        .grid-cell.agent::before {
            animation: pulse 2s infinite;
            display: inline-block;
        }

        /* Status text animation */
        @keyframes fadeInUp {
            from {
                opacity: 0;
                transform: translateY(10px);
            }

            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        #statusText {
            animation: fadeInUp 0.5s ease-out;
        }

        /* Improved scrollbar */
        ::-webkit-scrollbar {
            width: 8px;
            height: 8px;
        }

        ::-webkit-scrollbar-track {
            background: var(--background-alt);
        }

        ::-webkit-scrollbar-thumb {
            background: var(--text-light);
            border-radius: 4px;
        }

        ::-webkit-scrollbar-thumb:hover {
            background: var(--primary);
        }

        /* Accessibility improvements */
        .visually-hidden {
            position: absolute;
            width: 1px;
            height: 1px;
            padding: 0;
            margin: -1px;
            overflow: hidden;
            clip: rect(0, 0, 0, 0);
            white-space: nowrap;
            border-width: 0;
        }

        /* Skip to content link */
        .skip-link {
            position: absolute;
            top: -40px;
            left: 0;
            background: var(--primary);
            color: white;
            padding: 8px;
            z-index: 1000;
            transition: top 0.3s;
        }

        .skip-link:focus {
            top: 0;
        }

        /* Settings panel */
        .settings-panel {
            position: fixed;
            top: 0;
            right: -300px;
            width: 300px;
            height: 100vh;
            background: var(--background);
            box-shadow: -2px 0 10px var(--shadow);
            transition: right 0.3s ease;
            z-index: 1000;
            padding: 1.5rem;
            overflow-y: auto;
        }

        .settings-panel.open {
            right: 0;
        }

        .settings-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 1.5rem;
        }

        .settings-close {
            background: transparent;
            border: none;
            font-size: 1.5rem;
            cursor: pointer;
            color: var(--text);
        }

        .settings-section {
            margin-bottom: 1.5rem;
        }

        .settings-title {
            font-size: 1.25rem;
            margin-bottom: 1rem;
            color: var(--primary);
            border-bottom: 1px solid var(--border);
            padding-bottom: 0.5rem;
        }

        .settings-option {
            margin-bottom: 1rem;
        }

        .settings-label {
            display: block;
            margin-bottom: 0.5rem;
        }

        .settings-slider {
            width: 100%;
            height: 6px;
            -webkit-appearance: none;
            background: var(--background-alt);
            border-radius: 5px;
            outline: none;
        }

        .settings-slider::-webkit-slider-thumb {
            -webkit-appearance: none;
            appearance: none;
            width: 18px;
            height: 18px;
            border-radius: 50%;
            background: var(--primary);
            cursor: pointer;
        }

        .settings-slider::-moz-range-thumb {
            width: 18px;
            height: 18px;
            border-radius: 50%;
            background: var(--primary);
            cursor: pointer;
            border: none;
        }

        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 24px;
        }

        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }

        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: var(--text-light);
            transition: .4s;
            border-radius: 24px;
        }

        .slider:before {
            position: absolute;
            content: "";
            height: 16px;
            width: 16px;
            left: 4px;
            bottom: 4px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }

        input:checked+.slider {
            background-color: var(--primary);
        }

        input:focus+.slider {
            box-shadow: 0 0 1px var(--primary);
        }

        input:checked+.slider:before {
            transform: translateX(26px);
        }

        .flex-between {
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        /* New styles for policy visualization */
        .policy-arrow {
            font-size: 1.25rem;
            font-weight: bold;
        }

        .grid-cell[data-value]:after {
            content: attr(data-value);
            position: absolute;
            bottom: 2px;
            right: 2px;
            font-size: 0.625rem;
            color: var(--text-light);
        }

        /* Code snippet styles */
        .code-snippet {
            background-color: var(--background-alt);
            border-radius: 6px;
            padding: 1rem;
            margin: 1rem 0;
            font-family: 'Consolas', 'Monaco', 'Courier New', monospace;
            font-size: 0.9rem;
            overflow-x: auto;
            border-left: 4px solid var(--primary);
        }

        /* Search functionality */
        .search-input {
            width: 100%;
            padding: 0.75rem 1rem 0.75rem 2.5rem;
            border-radius: 6px;
            border: 1px solid var(--border);
            background-color: var(--background);
            color: var(--text);
            font-size: 0.9rem;
            transition: border-color 0.2s ease, box-shadow 0.2s ease;
            box-shadow: 0 2px 4px var(--shadow);
        }

        .search-input:focus {
            outline: none;
            border-color: var(--primary);
            box-shadow: 0 0 0 3px rgba(59, 130, 246, 0.25);
        }

        .search-icon {
            position: absolute;
            left: 0.75rem;
            top: 50%;
            transform: translateY(-50%);
            color: var(--text-light);
        }

        /* Comparison table */
        .comparison-table {
            width: 100%;
            border-collapse: collapse;
            margin: 1.5rem 0;
        }

        .comparison-table th,
        .comparison-table td {
            padding: 0.75rem 1rem;
            border: 1px solid var(--border);
            text-align: left;
        }

        .comparison-table th {
            background-color: var(--background-alt);
            font-weight: 600;
        }

        .comparison-table tr:nth-child(even) {
            background-color: rgba(0, 0, 0, 0.02);
        }

        .comparison-table tr:hover {
            background-color: rgba(59, 130, 246, 0.05);
        }

        /* Glossary styles */
        .glossary-term {
            font-weight: 600;
            color: var(--primary-dark);
            cursor: help;
            border-bottom: 1px dotted var(--primary-light);
            position: relative;
        }

        .glossary-term:hover::after {
            content: attr(data-definition);
            position: absolute;
            bottom: 100%;
            left: 50%;
            transform: translateX(-50%);
            background-color: var(--background);
            color: var(--text);
            padding: 0.75rem;
            border-radius: 6px;
            box-shadow: 0 4px 12px var(--shadow);
            width: 250px;
            z-index: 10;
            font-weight: normal;
            font-size: 0.85rem;
            text-align: center;
        }

        /* Q-Learning Formula Styling */
        .q-learning-formula {
            background-color: var(--background-alt);
            border-radius: 8px;
            padding: 1.5rem;
            margin: 1.5rem 0;
            border-left: 4px solid var(--primary);
            font-family: 'Consolas', 'Monaco', 'Courier New', monospace;
            box-shadow: 0 2px 8px var(--shadow);
        }

        .q-learning-formula .formula {
            text-align: center;
            font-size: 1.1rem;
            font-weight: 500;
            margin-bottom: 1rem;
            padding-bottom: 0.5rem;
            border-bottom: 1px dashed var(--border);
            line-height: 1.8;
        }

        .q-learning-formula .formula sub,
        .q-learning-formula .formula sup {
            font-size: 0.75em;
        }

        .q-learning-formula .variables {
            display: grid;
            grid-template-columns: auto 1fr;
            gap: 0.5rem 1rem;
        }

        .q-learning-formula .variable-name {
            font-weight: bold;
            color: var(--primary-dark);
        }

        .q-learning-formula .variable-description {
            color: var(--text);
        }

        /* Improved pseudocode styling */
        .pseudocode {
            background-color: var(--background-alt);
            border-radius: 8px;
            padding: 1.25rem;
            margin: 1.5rem 0;
            font-family: 'Consolas', 'Monaco', 'Courier New', monospace;
            font-size: 0.95rem;
            counter-reset: line;
            white-space: nowrap;
            overflow-x: auto;
            border-left: 4px solid var(--primary);
            box-shadow: 0 2px 8px var(--shadow);
            line-height: 1.5;
        }

        .pseudocode-line {
            display: block;
            position: relative;
            padding-left: 2.5rem;
            min-height: 1.5rem;
            margin-bottom: 0.25rem;
        }

        .pseudocode-line::before {
            content: counter(line);
            counter-increment: line;
            position: absolute;
            left: 0;
            width: 2rem;
            text-align: right;
            color: var(--text-light);
            border-right: 1px solid var(--border);
            padding-right: 0.5rem;
        }

        /* Floating action button */
        .fab {
            position: fixed;
            bottom: 2rem;
            right: 2rem;
            width: 3.5rem;
            height: 3.5rem;
            border-radius: 50%;
            background-color: var(--primary);
            color: white;
            display: flex;
            align-items: center;
            justify-content: center;
            box-shadow: 0 4px 10px rgba(0, 0, 0, 0.15);
            cursor: pointer;
            transition: background-color 0.2s ease, transform 0.2s ease, box-shadow 0.2s ease;
            z-index: 99;
        }

        .fab:hover {
            background-color: var(--primary-dark);
            transform: translateY(-2px);
            box-shadow: 0 6px 15px rgba(0, 0, 0, 0.2);
        }

        .fab:active {
            transform: translateY(0);
        }

        .fab svg {
            width: 1.5rem;
            height: 1.5rem;
        }

        /* Chat assistant */
        .chat-container {
            position: fixed;
            bottom: 5.5rem;
            right: 2rem;
            width: 350px;
            height: 450px;
            background-color: var(--background);
            border-radius: 12px;
            box-shadow: 0 5px 20px rgba(0, 0, 0, 0.2);
            display: flex;
            flex-direction: column;
            overflow: hidden;
            z-index: 98;
            transition: transform 0.3s ease, opacity 0.3s ease;
            transform: translateY(20px);
            opacity: 0;
            pointer-events: none;
        }

        .chat-container.active {
            transform: translateY(0);
            opacity: 1;
            pointer-events: all;
        }

        .chat-header {
            background-color: var(--primary);
            color: white;
            padding: 1rem;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        .chat-header h3 {
            margin: 0;
            color: white;
        }

        .chat-close {
            background: transparent;
            border: none;
            color: white;
            cursor: pointer;
            font-size: 1.25rem;
        }

        .chat-messages {
            flex: 1;
            padding: 1rem;
            overflow-y: auto;
            display: flex;
            flex-direction: column;
            gap: 0.5rem;
        }

        .chat-message {
            max-width: 80%;
            padding: 0.75rem;
            border-radius: 12px;
            line-height: 1.4;
        }

        .chat-message.user {
            align-self: flex-end;
            background-color: var(--primary);
            color: white;
            border-bottom-right-radius: 4px;
        }

        .chat-message.bot {
            align-self: flex-start;
            background-color: var(--background-alt);
            color: var(--text);
            border-bottom-left-radius: 4px;
        }

        .chat-input-container {
            padding: 1rem;
            border-top: 1px solid var(--border);
            display: flex;
            gap: 0.5rem;
        }

        .chat-input {
            flex: 1;
            padding: 0.75rem;
            border-radius: 20px;
            border: 1px solid var(--border);
            background-color: var(--background-alt);
            color: var(--text);
            resize: none;
        }

        .chat-input:focus {
            outline: none;
            border-color: var(--primary);
        }

        .chat-send {
            width: 40px;
            height: 40px;
            border-radius: 50%;
            background-color: var(--primary);
            color: white;
            border: none;
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: center;
            transition: background-color 0.2s ease;
        }

        .chat-send:hover {
            background-color: var(--primary-dark);
        }

        /* Print styles */
        @media print {

            header,
            .sidebar,
            .settings-panel,
            .fab,
            .chat-container,
            .demo-controls,
            .theme-toggle,
            .skip-link {
                display: none !important;
            }

            .container {
                display: block;
                max-width: 100%;
                padding: 0;
            }

            .main-content {
                width: 100%;
            }

            .card {
                break-inside: avoid;
                margin-bottom: 1rem;
                box-shadow: none;
                border: 1px solid #ddd;
            }

            .card:hover {
                transform: none;
                box-shadow: none;
            }

            body {
                background-color: white;
                font-size: 12pt;
            }

            a {
                text-decoration: none;
                color: black;
            }

            .card-header {
                background-color: #f0f0f0 !important;
                color: black !important;
            }
        }

        /* Search highlight styles */
        .search-highlight {
            background-color: rgba(59, 130, 246, 0.3);
            padding: 0 2px;
            border-radius: 2px;
            box-shadow: 0 0 0 1px rgba(59, 130, 246, 0.1);
            transition: background-color 0.3s ease;
        }

        .search-highlight:hover {
            background-color: rgba(59, 130, 246, 0.5);
        }

        /* Button focus styles for better accessibility */
        button:focus-visible,
        .btn:focus-visible {
            outline: 2px solid var(--primary);
            outline-offset: 2px;
        }

        /* Additional button hover effects */
        .btn:hover {
            background-color: var(--primary-dark);
            transform: translateY(-2px);
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
            transition: all 0.2s ease;
        }

        /* Make sure the settings button stays fixed in place */
        #settingsBtn {
            margin-top: auto;
            width: 100%;
            box-shadow: 0 2px 4px var(--shadow);
        }

        /* Parameter Controls Styles */
        .param-controls {
            margin-top: 1rem;
            padding: 1.25rem;
            background-color: var(--background-alt);
            border-radius: 8px;
            border: 1px solid var(--border);
            box-shadow: 0 2px 6px var(--shadow);
            transition: all 0.3s ease;
        }

        .param-controls h4 {
            margin-bottom: 1rem;
            color: var(--primary);
            border-bottom: 1px solid var(--border);
            padding-bottom: 0.5rem;
        }

        .param-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 1.25rem;
        }

        .param-item {
            display: flex;
            flex-direction: column;
        }

        .param-label {
            display: flex;
            justify-content: space-between;
            margin-bottom: 0.5rem;
            font-weight: 500;
        }

        .param-value {
            color: var(--primary);
            font-weight: bold;
            background-color: var(--background);
            padding: 0.1rem 0.5rem;
            border-radius: 4px;
            min-width: 40px;
            text-align: center;
        }

        .param-slider {
            width: 100%;
            height: 6px;
            -webkit-appearance: none;
            background: var(--background);
            border-radius: 5px;
            outline: none;
            border: 1px solid var(--border);
            box-shadow: inset 0 1px 3px rgba(0, 0, 0, 0.1);
        }

        .param-slider::-webkit-slider-thumb {
            -webkit-appearance: none;
            appearance: none;
            width: 18px;
            height: 18px;
            border-radius: 50%;
            background: var(--primary);
            cursor: pointer;
            box-shadow: 0 1px 3px rgba(0, 0, 0, 0.2);
            transition: background 0.2s ease;
        }

        .param-slider::-moz-range-thumb {
            width: 18px;
            height: 18px;
            border-radius: 50%;
            background: var(--primary);
            cursor: pointer;
            border: none;
            box-shadow: 0 1px 3px rgba(0, 0, 0, 0.2);
            transition: background 0.2s ease;
        }

        .param-slider::-webkit-slider-thumb:hover {
            background: var(--primary-dark);
            transform: scale(1.1);
        }

        .param-slider::-moz-range-thumb:hover {
            background: var(--primary-dark);
            transform: scale(1.1);
        }

        /* Parameter animation */
        @keyframes paramUpdate {
            0% {
                background-color: var(--primary-light);
                color: white;
            }

            100% {
                background-color: var(--background);
                color: var(--primary);
            }
        }

        .param-value-updated {
            animation: paramUpdate 1s ease;
        }

        /* Responsive adjustments */
        @media (max-width: 600px) {
            .param-grid {
                grid-template-columns: 1fr;
                gap: 1rem;
            }
        }
    </style>
</head>

<body>
    <a href="#main-content" class="skip-link">Skip to content</a>

    <header>
        <h1>Reinforcement Learning Explained</h1>
        <p>Understanding the foundations and applications of this powerful machine learning approach</p>
        <button id="themeToggle" class="theme-toggle" aria-label="Toggle dark mode">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
                stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                <circle cx="12" cy="12" r="5"></circle>
                <line x1="12" y1="1" x2="12" y2="3"></line>
                <line x1="12" y1="21" x2="12" y2="23"></line>
                <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                <line x1="1" y1="12" x2="3" y2="12"></line>
                <line x1="21" y1="12" x2="23" y2="12"></line>
                <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
            </svg>
            <span id="themeText">Dark Mode</span>
        </button>
    </header>

    <div class="container">
        <aside class="sidebar">
            <div class="search-container">
                <input type="text" id="searchInput" class="search-input" placeholder="Search..."
                    aria-label="Search content">
                <svg class="search-icon" xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24"
                    fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                    <circle cx="11" cy="11" r="8"></circle>
                    <line x1="21" y1="21" x2="16.65" y2="16.65"></line>
                </svg>
            </div>

            <div class="sidebar-nav">
                <div class="sidebar-nav-header">
                    Table of Contents
                </div>
                <ul>
                    <li><a href="#introduction" class="active" aria-current="page">Introduction</a></li>
                    <li><a href="#key-concepts">Key Concepts</a></li>
                    <li><a href="#algorithms">Popular Algorithms</a></li>
                    <li><a href="#applications">Real-world Applications</a></li>
                    <li><a href="#demo">Interactive Demo</a></li>
                    <li><a href="#resources">Further Resources</a></li>
                </ul>
            </div>
            <button id="settingsBtn" class="btn" aria-label="Open settings">
                <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none"
                    stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"
                    style="margin-right: 0.5rem; vertical-align: -3px;">
                    <circle cx="12" cy="12" r="3"></circle>
                    <path
                        d="M19.4 15a1.65 1.65 0 0 0 .33 1.82l.06.06a2 2 0 0 1 0 2.83 2 2 0 0 1-2.83 0l-.06-.06a1.65 1.65 0 0 0-1.82-.33 1.65 1.65 0 0 0-1 1.51V21a2 2 0 0 1-2 2 2 2 0 0 1-2-2v-.09A1.65 1.65 0 0 0 9 19.4a1.65 1.65 0 0 0-1.82.33l-.06.06a2 2 0 0 1-2.83 0 2 2 0 0 1 0-2.83l.06-.06a1.65 1.65 0 0 0 .33-1.82 1.65 1.65 0 0 0-1.51-1H3a2 2 0 0 1-2-2 2 2 0 0 1 2-2h.09A1.65 1.65 0 0 0 4.6 9a1.65 1.65 0 0 0-.33-1.82l-.06-.06a2 2 0 0 1 0-2.83 2 2 0 0 1 2.83 0l.06.06a1.65 1.65 0 0 0 1.82.33H9a1.65 1.65 0 0 0 1-1.51V3a2 2 0 0 1 2-2 2 2 0 0 1 2 2v.09a1.65 1.65 0 0 0 1 1.51 1.65 1.65 0 0 0 1.82-.33l.06-.06a2 2 0 0 1 2.83 0 2 2 0 0 1 0 2.83l-.06.06a1.65 1.65 0 0 0-.33 1.82V9a1.65 1.65 0 0 0 1.51 1H21a2 2 0 0 1 2 2 2 2 0 0 1-2 2h-.09a1.65 1.65 0 0 0-1.51 1z">
                    </path>
                </svg>
                Settings
            </button>
        </aside>

        <main class="main-content" id="main-content">
            <section class="card" id="introduction">
                <div class="card-header">
                    <h2>Introduction to Reinforcement Learning</h2>
                </div>
                <div class="card-body">
                    <p>Reinforcement Learning (RL) is a type of machine learning where an <strong>agent</strong> learns
                        to make decisions by taking actions in an environment to maximize some notion of cumulative
                        reward. Unlike supervised learning, RL doesn't rely on labeled training data. Instead, it learns
                        through trial and error interactions with its environment.</p>

                    <div class="info-box">
                        <p><strong>Key Distinction:</strong> While supervised learning learns from examples provided by
                            a knowledgeable external supervisor, reinforcement learning uses direct interaction with the
                            environment to discover optimal behaviors through exploration and exploitation of knowledge.
                        </p>
                    </div>

                    <p>The fundamental RL process works as follows:</p>
                    <ol>
                        <li>The agent observes the current state of the environment</li>
                        <li>Based on this state, the agent selects an action</li>
                        <li>The environment transitions to a new state</li>
                        <li>The environment provides a reward signal to the agent</li>
                        <li>The agent uses this feedback to improve its policy for future actions</li>
                    </ol>

                    <div style="text-align: center; margin: 2rem 0;">
                        <svg class="rl-diagram" width="600" height="300" viewBox="0 0 600 300"
                            aria-labelledby="rl-diagram-title" role="img">
                            <title id="rl-diagram-title">Reinforcement Learning Process Diagram</title>
                            <!-- Environment -->
                            <rect x="50" y="50" width="500" height="200" rx="10" fill="#f3f4f6" stroke="#d1d5db"
                                stroke-width="2" />
                            <text x="300" y="80" text-anchor="middle" font-size="20" fill="#1f2937">Environment</text>

                            <!-- Agent -->
                            <rect x="250" y="130" width="100" height="60" rx="8" fill="#2563eb" stroke="#1d4ed8"
                                stroke-width="2" />
                            <text x="300" y="165" text-anchor="middle" font-size="16" fill="white">Agent</text>

                            <!-- State -->
                            <rect x="100" y="140" width="80" height="40" rx="5" fill="#10b981" stroke="#059669"
                                stroke-width="2" />
                            <text x="140" y="165" text-anchor="middle" font-size="14" fill="white">State</text>

                            <!-- Reward -->
                            <rect x="420" y="140" width="80" height="40" rx="5" fill="#f59e0b" stroke="#d97706"
                                stroke-width="2" />
                            <text x="460" y="165" text-anchor="middle" font-size="14" fill="white">Reward</text>

                            <!-- Arrows -->
                            <path d="M180 160 L245 160" stroke="#6b7280" stroke-width="2"
                                marker-end="url(#arrowhead)" />
                            <path d="M350 160 L415 160" stroke="#6b7280" stroke-width="2"
                                marker-end="url(#arrowhead)" />
                            <path d="M460 180 C460,220 140,220 140,180" stroke="#6b7280" stroke-width="2"
                                marker-end="url(#arrowhead)" />

                            <!-- Arrow Definitions -->
                            <defs>
                                <marker id="arrowhead" markerWidth="10" markerHeight="7" refX="9" refY="3.5"
                                    orient="auto">
                                    <polygon points="0 0, 10 3.5, 0 7" fill="#6b7280" />
                                </marker>
                            </defs>

                            <!-- Labels -->
                            <text x="210" y="150" text-anchor="middle" font-size="12" fill="#6b7280">Observes</text>
                            <text x="380" y="150" text-anchor="middle" font-size="12" fill="#6b7280">Receives</text>
                            <text x="300" y="230" text-anchor="middle" font-size="12" fill="#6b7280">Takes Action &
                                Updates Policy</text>
                        </svg>
                    </div>

                    <p>This iterative process allows the agent to learn which actions yield the highest rewards in
                        different states, ultimately developing a strategy (<span class="glossary-term"
                            data-definition="A policy is the agent's strategy for selecting actions in different states to maximize future rewards.">policy</span>)
                        that maximizes long-term rewards.</p>

                    <div class="tooltip">
                        <p><strong>Hover for a simple analogy:</strong> Think of reinforcement learning like training a
                            dog. üêï</p>
                        <span class="tooltiptext">When training a dog, you reward good behaviors (treats, praise) and
                            discourage bad ones. Over time, the dog learns which behaviors lead to rewards. Similarly,
                            RL agents learn through rewards and penalties which actions are beneficial in different
                            situations.</span>
                    </div>

                    <h3>Comparing RL with Other Machine Learning Approaches</h3>

                    <table class="comparison-table">
                        <thead>
                            <tr>
                                <th>Learning Paradigm</th>
                                <th>Training Data</th>
                                <th>Learning Process</th>
                                <th>Example Applications</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>Supervised Learning</td>
                                <td>Labeled data (inputs paired with correct outputs)</td>
                                <td>Learn mapping from inputs to outputs</td>
                                <td>Image classification, spam detection, speech recognition</td>
                            </tr>
                            <tr>
                                <td>Unsupervised Learning</td>
                                <td>Unlabeled data</td>
                                <td>Find patterns or structure in data</td>
                                <td>Clustering, dimensionality reduction, anomaly detection</td>
                            </tr>
                            <tr>
                                <td>Reinforcement Learning</td>
                                <td>No explicit training data, learns from interaction</td>
                                <td>Learn optimal actions through trial and error</td>
                                <td>Game playing, robotics, resource management</td>
                            </tr>
                        </tbody>
                    </table>

                    <h3>Simple Example: The Multi-Armed Bandit</h3>

                    <p>A classic introductory problem in RL is the <span class="glossary-term"
                            data-definition="A problem where an agent must choose between multiple actions (like slot machines or 'bandits'), each with unknown reward probabilities, to maximize total reward over time.">multi-armed
                            bandit</span>. Imagine a slot machine with multiple arms, each giving rewards with different
                        (unknown) probabilities. Your goal is to maximize your total reward by choosing which arm to
                        pull.</p>

                    <div class="info-box">
                        <p>This simple problem illustrates the fundamental <strong>exploration-exploitation
                                dilemma</strong> in RL:</p>
                        <ul>
                            <li><strong>Exploration:</strong> Trying different arms to learn their reward distributions
                            </li>
                            <li><strong>Exploitation:</strong> Repeatedly pulling the arm that you believe gives the
                                highest reward</li>
                        </ul>
                        <p>Too much exploration wastes pulls on suboptimal arms. Too much exploitation might miss
                            discovering the truly optimal arm.</p>
                    </div>

                    <p>While the multi-armed bandit is simpler than full RL (it has no state transitions), it introduces
                        key concepts that extend to more complex RL problems.</p>
                </div>
            </section>

            <section class="card" id="key-concepts">
                <div class="card-header">
                    <h2>Key Concepts in Reinforcement Learning</h2>
                </div>
                <div class="card-body">
                    <div class="tabs" role="tablist">
                        <div class="tab active" role="tab" aria-selected="true" tabindex="0" data-tab="components"
                            id="tab-components">Components</div>
                        <div class="tab" role="tab" aria-selected="false" tabindex="-1" data-tab="exploration"
                            id="tab-exploration">Exploration vs. Exploitation</div>
                        <div class="tab" role="tab" aria-selected="false" tabindex="-1" data-tab="markov"
                            id="tab-markov">Markov Decision Process</div>
                        <div class="tab" role="tab" aria-selected="false" tabindex="-1" data-tab="value" id="tab-value">
                            Value Functions</div>
                    </div>

                    <div class="tab-content active" id="components" role="tabpanel" aria-labelledby="tab-components">
                        <h3>Core Components</h3>
                        <ul>
                            <li><strong>Agent:</strong> The learner or decision-maker that interacts with the
                                environment.</li>
                            <li><strong>Environment:</strong> The world in which the agent exists and operates.</li>
                            <li><strong>State (S):</strong> A specific situation or configuration of the environment.
                            </li>
                            <li><strong>Action (A):</strong> A move or decision that the agent can make.</li>
                            <li><strong>Reward (R):</strong> Feedback signal indicating the immediate benefit of an
                                action.</li>
                            <li><strong>Policy (œÄ):</strong> The agent's strategy for selecting actions in given states.
                            </li>
                            <li><strong>Value Function (V):</strong> Estimates the long-term desirability of states.
                            </li>
                            <li><strong>Q-Function (Q):</strong> Estimates the value of state-action pairs.</li>
                        </ul>

                        <h3>Types of Reinforcement Learning</h3>

                        <div class="info-box">
                            <h4>Model-based vs. Model-free RL</h4>
                            <ul>
                                <li><strong>Model-based:</strong> The agent builds an internal model of how the
                                    environment works (transition and reward functions) and uses it for planning.</li>
                                <li><strong>Model-free:</strong> The agent learns directly from experience without
                                    explicitly modeling the environment.</li>
                            </ul>
                        </div>

                        <div class="info-box">
                            <h4>On-policy vs. Off-policy Learning</h4>
                            <ul>
                                <li><strong>On-policy:</strong> The agent learns the value of the policy it's currently
                                    following.</li>
                                <li><strong>Off-policy:</strong> The agent learns about a target policy while following
                                    a different behavior policy (enabling learning from stored experiences or
                                    demonstrations).</li>
                            </ul>
                        </div>

                        <h3>The Reinforcement Learning Problem</h3>
                        <p>At its core, the goal of reinforcement learning is to find a policy œÄ that maximizes the
                            expected cumulative (often discounted) reward over time:</p>

                        <div class="code-snippet">
                            E[‚àë Œ≥·µó¬∑r‚Çú]
                        </div>

                        <p>Where:</p>
                        <ul>
                            <li>E is the expected value</li>
                            <li>‚àë is the sum over time steps</li>
                            <li>Œ≥ (gamma) is the discount factor (0 ‚â§ Œ≥ ‚â§ 1)</li>
                            <li>r‚Çú is the reward at time t</li>
                        </ul>

                        <p>The discount factor Œ≥ determines how much the agent values future rewards compared to
                            immediate ones. A value close to 0 makes the agent "myopic" (focused on immediate rewards),
                            while a value close to 1 makes it "far-sighted" (considering long-term rewards).</p>
                    </div>

                    <div class="tab-content" id="exploration" role="tabpanel" aria-labelledby="tab-exploration">
                        <h3>Exploration vs. Exploitation</h3>
                        <p>One of the fundamental challenges in reinforcement learning is balancing:</p>
                        <ul>
                            <li><strong>Exploration:</strong> Trying new actions to discover better strategies.</li>
                            <li><strong>Exploitation:</strong> Using known good strategies to maximize immediate reward.
                            </li>
                        </ul>
                        <p>Too much exploration may prevent the agent from maximizing rewards, while too much
                            exploitation might cause the agent to miss better strategies.</p>

                        <div class="info-box">
                            <p><strong>Common approaches to balance exploration and exploitation include:</strong></p>
                            <ul>
                                <li><strong>Œµ-greedy:</strong> Choose the best-known action most of the time, but
                                    occasionally (with probability Œµ) select a random action.</li>
                                <li><strong>Softmax:</strong> Select actions probabilistically based on their estimated
                                    values.</li>
                                <li><strong>Upper Confidence Bound (UCB):</strong> Select actions that maximize an upper
                                    confidence bound on potential reward.</li>
                                <li><strong>Thompson Sampling:</strong> Sample from a posterior distribution of expected
                                    rewards and choose the action with the highest sampled value.</li>
                            </ul>
                        </div>

                        <h3>Œµ-greedy Algorithm</h3>
                        <div class="pseudocode">
                            <span class="pseudocode-line">function SelectAction(state):</span>
                            <span class="pseudocode-line"> p = random number between 0 and 1</span>
                            <span class="pseudocode-line"> if p < Œµ then</span>
                                    <span class="pseudocode-line"> return random action</span>
                                    <span class="pseudocode-line"> else</span>
                                    <span class="pseudocode-line"> return action with highest Q-value in state</span>
                                    <span class="pseudocode-line"> end if</span>
                                    <span class="pseudocode-line">end function</span>
                        </div>

                        <p>The exploration-exploitation dilemma is often compared to the "multi-armed bandit" problem:
                            imagine a gambler at a row of slot machines (bandits), each with different payouts. The
                            gambler must decide which machines to play to maximize total reward, balancing between
                            exploring new machines and exploiting known high-paying ones.</p>

                        <h3>Exploration Strategies in Deep RL</h3>
                        <p>In complex environments with large state spaces, basic exploration strategies may not be
                            sufficient. Advanced methods include:</p>

                        <ul>
                            <li><strong>Intrinsic Motivation:</strong> The agent receives additional rewards for
                                exploring novel or uncertain states.</li>
                            <li><strong>Parameter Space Noise:</strong> Adding noise to the parameters of the policy
                                network to encourage diverse behavior.</li>
                            <li><strong>Count-based Exploration:</strong> Encouraging visits to rarely-seen states by
                                tracking visit counts.</li>
                            <li><strong>Curiosity-driven Exploration:</strong> Rewarding the agent for actions that lead
                                to unpredictable outcomes.</li>
                        </ul>
                    </div>

                    <div class="tab-content" id="markov" role="tabpanel" aria-labelledby="tab-markov">
                        <h3>Markov Decision Process (MDP)</h3>
                        <p>MDPs provide the mathematical framework for modeling decision-making in RL. An MDP consists
                            of:</p>
                        <ul>
                            <li><strong>A set of states S</strong></li>
                            <li><strong>A set of actions A</strong></li>
                            <li><strong>Transition probabilities P(s'|s,a)</strong> - The probability of transitioning
                                to state s' when taking action a in state s</li>
                            <li><strong>Reward function R(s,a,s')</strong> - The reward received after transitioning
                                from s to s' via action a</li>
                            <li><strong>Discount factor Œ≥</strong> - A parameter between 0 and 1 that determines how
                                much the agent values future rewards</li>
                        </ul>
                        <p>The "Markov" property means that the future state depends only on the current state and
                            action, not on the history of previous states and actions.</p>

                        <div class="info-box">
                            <p><strong>Formal Definition:</strong> A state s has the Markov property if and only if:</p>
                            <p>P(s<sub>t+1</sub> | s<sub>t</sub>, a<sub>t</sub>) = P(s<sub>t+1</sub> | s<sub>t</sub>,
                                a<sub>t</sub>, s<sub>t-1</sub>, a<sub>t-1</sub>, ..., s<sub>0</sub>, a<sub>0</sub>)</p>
                            <p>In other words, knowing the current state is sufficient to predict the next state; the
                                history of how we got to the current state is irrelevant.</p>
                        </div>

                        <h3>Types of MDPs</h3>

                        <table class="comparison-table">
                            <thead>
                                <tr>
                                    <th>Type</th>
                                    <th>Description</th>
                                    <th>Example</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>Finite MDP</td>
                                    <td>Has finite state and action spaces</td>
                                    <td>Grid world navigation</td>
                                </tr>
                                <tr>
                                    <td>Infinite MDP</td>
                                    <td>Has infinite state and/or action spaces</td>
                                    <td>Continuous control problems (e.g., robotics)</td>
                                </tr>
                                <tr>
                                    <td>Partially Observable MDP (POMDP)</td>
                                    <td>Agent cannot directly observe the true state</td>
                                    <td>Robot with limited sensor data</td>
                                </tr>
                                <tr>
                                    <td>Multi-agent MDP</td>
                                    <td>Multiple agents interact in the same environment</td>
                                    <td>Multiplayer games, traffic systems</td>
                                </tr>
                            </tbody>
                        </table>

                        <h3>Solving MDPs</h3>
                        <p>The solution to an MDP is an optimal policy œÄ* that maximizes the expected cumulative reward.
                            Methods for solving MDPs include:</p>

                        <ul>
                            <li><strong>Dynamic Programming:</strong> Methods like value iteration and policy iteration
                                (when the MDP is fully known)</li>
                            <li><strong>Monte Carlo Methods:</strong> Learning from complete episodes of experience</li>
                            <li><strong>Temporal Difference Learning:</strong> Learning from partial episodes by
                                bootstrapping</li>
                            <li><strong>Function Approximation:</strong> Using neural networks to handle large state
                                spaces</li>
                        </ul>
                    </div>

                    <div class="tab-content" id="value" role="tabpanel" aria-labelledby="tab-value">
                        <h3>Value Functions</h3>
                        <p>Value functions estimate how good it is for an agent to be in a particular state or to take a
                            specific action in a given state:</p>
                        <ul>
                            <li><strong>State-Value Function V(s):</strong> The expected return starting from state s
                                and following policy œÄ.</li>
                            <li><strong>Action-Value Function Q(s,a):</strong> The expected return starting from state
                                s, taking action a, and then following policy œÄ.</li>
                        </ul>
                        <p>These functions help the agent evaluate which states are valuable to reach and which actions
                            are beneficial to take.</p>

                        <div class="info-box">
                            <p><strong>The Bellman equations provide recursive definitions for these value
                                    functions:</strong></p>
                            <p><strong>V<sub>œÄ</sub>(s) = E<sub>œÄ</sub>[R<sub>t+1</sub> +
                                    Œ≥V<sub>œÄ</sub>(S<sub>t+1</sub>) | S<sub>t</sub>=s]</strong></p>
                            <p><strong>Q<sub>œÄ</sub>(s,a) = E<sub>œÄ</sub>[R<sub>t+1</sub> +
                                    Œ≥Q<sub>œÄ</sub>(S<sub>t+1</sub>, A<sub>t+1</sub>) | S<sub>t</sub>=s,
                                    A<sub>t</sub>=a]</strong></p>
                            <p>For optimal policies, we have the Bellman optimality equations:</p>
                            <p><strong>V*(s) = max<sub>a</sub> E[R<sub>t+1</sub> + Œ≥V*(S<sub>t+1</sub>) |
                                    S<sub>t</sub>=s, A<sub>t</sub>=a]</strong></p>
                            <p><strong>Q*(s,a) = E[R<sub>t+1</sub> + Œ≥ max<sub>a'</sub> Q*(S<sub>t+1</sub>,a') |
                                    S<sub>t</sub>=s, A<sub>t</sub>=a]</strong></p>
                            <p>These equations form the basis for many RL algorithms.</p>
                        </div>

                        <h3>Advantages of Different Value Functions</h3>

                        <table class="comparison-table">
                            <thead>
                                <tr>
                                    <th>Value Function</th>
                                    <th>Advantages</th>
                                    <th>Disadvantages</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>State-Value (V)</td>
                                    <td>
                                        - More compact (one value per state)<br>
                                        - Useful for evaluating policies<br>
                                        - Can be more sample-efficient
                                    </td>
                                    <td>
                                        - Requires model of environment for control<br>
                                        - Cannot directly select actions
                                    </td>
                                </tr>
                                <tr>
                                    <td>Action-Value (Q)</td>
                                    <td>
                                        - Directly enables action selection<br>
                                        - Model-free control is possible<br>
                                        - Easier to implement for many problems
                                    </td>
                                    <td>
                                        - More values to store/estimate<br>
                                        - May require more samples to learn
                                    </td>
                                </tr>
                                <tr>
                                    <td>Advantage (A)</td>
                                    <td>
                                        - Measures relative advantage of actions<br>
                                        - Reduces variance in policy gradients<br>
                                        - Combines benefits of V and Q
                                    </td>
                                    <td>
                                        - Requires estimating both V and Q<br>
                                        - More complex to implement
                                    </td>
                                </tr>
                            </tbody>
                        </table>

                        <h3>Approximating Value Functions</h3>
                        <p>In complex environments with large state spaces, it's impractical to store a separate value
                            for each state or state-action pair. Instead, we use function approximators:</p>

                        <ul>
                            <li><strong>Linear Function Approximation:</strong> V(s) ‚âà w<sup>T</sup>œÜ(s), where œÜ(s) is
                                a feature vector and w are weights</li>
                            <li><strong>Neural Networks:</strong> Deep neural networks can learn complex mappings from
                                states to values</li>
                            <li><strong>Decision Trees/Random Forests:</strong> Used in some approaches for
                                interpretable value functions</li>
                        </ul>

                        <p>Deep Q-Networks (DQN) and similar algorithms use neural networks to approximate Q-values,
                            enabling RL to tackle problems with high-dimensional state spaces like Atari games or
                            robotics.</p>
                    </div>
                </div>
            </section>

            <section class="card" id="algorithms">
                <div class="card-header">
                    <h2>Popular Reinforcement Learning Algorithms</h2>
                </div>
                <div class="card-body">
                    <p>Reinforcement learning algorithms can be broadly categorized into three types:</p>

                    <div class="algorithm-comparison">
                        <div class="card">
                            <div class="card-header">
                                <h3>Value-Based Methods</h3>
                            </div>
                            <div class="card-body">
                                <p>These methods focus on estimating the value of states or state-action pairs.</p>
                                <p><strong>Key Algorithms:</strong></p>
                                <ul>
                                    <li>Q-Learning</li>
                                    <li>SARSA</li>
                                    <li>Deep Q-Network (DQN)</li>
                                    <li>Double DQN</li>
                                    <li>Dueling DQN</li>
                                </ul>
                                <p><strong>Approach:</strong> Learn a value function and derive a policy from it.</p>
                                <div>
                                    <span class="badge">Value Estimation</span>
                                    <span class="badge">Off-Policy</span>
                                    <span class="badge">Function Approximation</span>
                                </div>
                            </div>
                        </div>

                        <div class="card">
                            <div class="card-header">
                                <h3>Policy-Based Methods</h3>
                            </div>
                            <div class="card-body">
                                <p>These methods directly optimize the policy without using a value function.</p>
                                <p><strong>Key Algorithms:</strong></p>
                                <ul>
                                    <li>REINFORCE</li>
                                    <li>Policy Gradient</li>
                                    <li>Trust Region Policy Optimization (TRPO)</li>
                                    <li>Soft Actor-Critic (SAC)</li>
                                </ul>
                                <p><strong>Approach:</strong> Directly search for optimal policy using gradient-based
                                    optimization.</p>
                                <div>
                                    <span class="badge">Direct Optimization</span>
                                    <span class="badge">On-Policy</span>
                                    <span class="badge">Continuous Actions</span>
                                </div>
                            </div>
                        </div>

                        <div class="card">
                            <div class="card-header">
                                <h3>Actor-Critic Methods</h3>
                            </div>
                            <div class="card-body">
                                <p>These methods combine value-based and policy-based approaches.</p>
                                <p><strong>Key Algorithms:</strong></p>
                                <ul>
                                    <li>Advantage Actor-Critic (A2C)</li>
                                    <li>Deep Deterministic Policy Gradient (DDPG)</li>
                                    <li>Proximal Policy Optimization (PPO)</li>
                                    <li>Asynchronous Advantage Actor-Critic (A3C)</li>
                                </ul>
                                <p><strong>Approach:</strong> Use a critic to estimate value function and an actor to
                                    update policy.</p>
                                <div>
                                    <span class="badge">Hybrid</span>
                                    <span class="badge">Reduced Variance</span>
                                    <span class="badge">Parallel Training</span>
                                </div>
                            </div>
                        </div>
                    </div>

                    <h3>Q-Learning: A Fundamental Algorithm</h3>

                    <p>Q-learning is one of the most popular and fundamental RL algorithms. It's a model-free,
                        off-policy algorithm that learns the optimal action-value function Q*(s,a).</p>

                    <div class="pseudocode">
                        <span class="pseudocode-line">Initialize Q(s,a) arbitrarily for all s, a</span>
                        <span class="pseudocode-line">For each episode:</span>
                        <span class="pseudocode-line"> Initialize state s</span>
                        <span class="pseudocode-line"> For each step of episode:</span>
                        <span class="pseudocode-line"> Choose action a from s using policy derived from Q (e.g.,
                            Œµ-greedy)</span>
                        <span class="pseudocode-line"> Take action a, observe reward r and next state s'</span>
                        <span class="pseudocode-line"> Q(s,a) ‚Üê Q(s,a) + Œ±[r + Œ≥¬∑max<sub>a'</sub> Q(s',a') -
                            Q(s,a)]</span>
                        <span class="pseudocode-line"> s ‚Üê s'</span>
                        <span class="pseudocode-line"> until s is terminal</span>
                    </div>

                    <div class="q-learning-formula">
                        <div class="formula">
                            Q(s,a) ‚Üê Q(s,a) + Œ±[r + Œ≥¬∑max<sub>a'</sub> Q(s',a') - Q(s,a)]
                        </div>
                        <div class="variables">
                            <span class="variable-name">Œ±:</span>
                            <span class="variable-description">Learning rate (how quickly new information overrides old
                                information)</span>

                            <span class="variable-name">Œ≥:</span>
                            <span class="variable-description">Discount factor (importance of future rewards vs.
                                immediate rewards)</span>

                            <span class="variable-name">r:</span>
                            <span class="variable-description">Immediate reward received after taking action a in state
                                s</span>

                            <span class="variable-name">s':</span>
                            <span class="variable-description">Next state reached after taking action a in state
                                s</span>

                            <span class="variable-name">max<sub>a'</sub> Q(s',a'):</span>
                            <span class="variable-description">Maximum Q-value possible in the next state s'</span>
                        </div>
                    </div>

                    <h3>Deep Reinforcement Learning</h3>
                    <p>Deep reinforcement learning combines reinforcement learning principles with deep neural networks
                        to handle high-dimensional state spaces. This has led to breakthroughs in:</p>
                    <ul>
                        <li>Game playing (AlphaGo, AlphaZero, MuZero)</li>
                        <li>Robotics control</li>
                        <li>Natural language processing</li>
                        <li>Autonomous vehicles</li>
                        <li>Healthcare applications</li>
                    </ul>

                    <div class="info-box">
                        <p><strong>Key Innovations in Deep RL:</strong></p>
                        <ul>
                            <li><strong>Experience Replay:</strong> Storing and reusing past experiences to break
                                correlations between consecutive training samples.</li>
                            <li><strong>Target Networks:</strong> Using separate networks for generating targets to
                                stabilize training.</li>
                            <li><strong>Distributional RL:</strong> Learning the distribution of returns instead of just
                                the expected value.</li>
                            <li><strong>Multi-Agent RL:</strong> Training multiple agents that interact with each other.
                            </li>
                            <li><strong>Meta-RL:</strong> Training agents that can quickly adapt to new tasks.</li>
                            <li><strong>Self-supervised RL:</strong> Learning useful representations without explicit
                                rewards.</li>
                        </ul>
                    </div>

                    <h3>Algorithm Selection Guide</h3>

                    <div class="info-box">
                        <p><strong>When to use different algorithms:</strong></p>
                        <ul>
                            <li><strong>Q-Learning/DQN:</strong> Discrete action spaces, when you want a deterministic
                                policy</li>
                            <li><strong>SARSA:</strong> When on-policy learning is important (e.g., in dangerous
                                environments)</li>
                            <li><strong>Policy Gradient:</strong> Continuous action spaces, stochastic policies</li>
                            <li><strong>PPO:</strong> Good general-purpose algorithm with stable learning</li>
                            <li><strong>SAC:</strong> Continuous action spaces with emphasis on exploration</li>
                            <li><strong>DDPG:</strong> Continuous control problems requiring deterministic policies</li>
                            <li><strong>A3C/A2C:</strong> When parallel training is beneficial</li>
                        </ul>
                    </div>
                </div>
            </section>

            <section class="card" id="applications">
                <div class="card-header">
                    <h2>Real-world Applications of Reinforcement Learning</h2>
                </div>
                <div class="card-body">
                    <p>Reinforcement learning has been successfully applied to a wide range of domains:</p>

                    <h3>Game Playing</h3>
                    <p>RL has achieved superhuman performance in:</p>
                    <ul>
                        <li><strong>Board games:</strong> Chess, Go, Shogi</li>
                        <li><strong>Video games:</strong> Atari games, StarCraft II, Dota 2</li>
                        <li><strong>Card games:</strong> Poker (Heads-up No-limit Texas Hold'em)</li>
                    </ul>

                    <h3>Robotics and Control</h3>
                    <ul>
                        <li>Robot locomotion and manipulation</li>
                        <li>Autonomous vehicles</li>
                        <li>Drone navigation and control</li>
                        <li>Industrial automation and manufacturing</li>
                        <li>Soft robotics and adaptive control systems</li>
                    </ul>

                    <h3>Business and Finance</h3>
                    <ul>
                        <li>Automated trading strategies</li>
                        <li>Portfolio management and optimization</li>
                        <li>Dynamic pricing models</li>
                        <li>Supply chain and resource allocation</li>
                        <li>Customer relationship management</li>
                    </ul>

                    <h3>Healthcare</h3>
                    <ul>
                        <li>Personalized treatment recommendations</li>
                        <li>Drug discovery and development</li>
                        <li>Medical image analysis and diagnostics</li>
                        <li>Clinical trial design and optimization</li>
                        <li>Healthcare resource management</li>
                    </ul>

                    <h3>Energy Management</h3>
                    <ul>
                        <li>Smart grid optimization</li>
                        <li>HVAC control for energy efficiency</li>
                        <li>Renewable energy integration</li>
                        <li>Demand response systems</li>
                        <li>Battery management and storage optimization</li>
                    </ul>

                    <h3>Recommendation Systems</h3>
                    <ul>
                        <li>Content personalization</li>
                        <li>Ad targeting and placement</li>
                        <li>User engagement optimization</li>
                        <li>Sequential recommendation</li>
                        <li>Diversified content discovery</li>
                    </ul>

                    <div class="info-box">
                        <p><strong>Case Study: DeepMind's AlphaGo</strong></p>
                        <p>In 2016, DeepMind's AlphaGo defeated world champion Lee Sedol in the complex board game Go.
                            This was a significant achievement because:</p>
                        <ul>
                            <li>Go has approximately 10<sup>170</sup> possible board positions (more than atoms in the
                                observable universe)</li>
                            <li>Traditional game-playing algorithms like minimax with alpha-beta pruning weren't
                                feasible</li>
                            <li>AlphaGo combined deep neural networks with reinforcement learning techniques</li>
                            <li>It learned both from human expert games and from self-play</li>
                            <li>Later versions (AlphaGo Zero and AlphaZero) learned entirely through self-play without
                                human data</li>
                        </ul>
                        <p>This demonstrated the potential of reinforcement learning to solve extremely complex problems
                            that were previously thought to require human intuition.</p>
                    </div>

                    <h3>Emerging Applications</h3>

                    <div class="algorithm-comparison">
                        <div class="card">
                            <div class="card-header">
                                <h3>Natural Language Processing</h3>
                            </div>
                            <div class="card-body">
                                <ul>
                                    <li>Dialogue systems and chatbots</li>
                                    <li>Text summarization</li>
                                    <li>Machine translation</li>
                                    <li>Content generation</li>
                                    <li>Information extraction</li>
                                </ul>
                                <div>
                                    <span class="badge">Human Feedback</span>
                                    <span class="badge">Language Models</span>
                                </div>
                            </div>
                        </div>

                        <div class="card">
                            <div class="card-header">
                                <h3>Scientific Research</h3>
                            </div>
                            <div class="card-body">
                                <ul>
                                    <li>Protein folding (AlphaFold)</li>
                                    <li>Materials discovery</li>
                                    <li>Quantum experiments</li>
                                    <li>Particle physics</li>
                                    <li>Astronomical data analysis</li>
                                </ul>
                                <div>
                                    <span class="badge">Simulation-Based</span>
                                    <span class="badge">Multi-objective</span>
                                </div>
                            </div>
                        </div>

                        <div class="card">
                            <div class="card-header">
                                <h3>Social Good</h3>
                            </div>
                            <div class="card-body">
                                <ul>
                                    <li>Wildlife conservation</li>
                                    <li>Disaster response</li>
                                    <li>Education personalization</li>
                                    <li>Public health interventions</li>
                                    <li>Traffic management</li>
                                </ul>
                                <div>
                                    <span class="badge">Resource Allocation</span>
                                    <span class="badge">Multi-agent Systems</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </section>

            <section class="card" id="demo">
                <div class="card-header">
                    <h2>Interactive RL Demo: Grid World</h2>
                </div>
                <div class="card-body">
                    <p>This simple demonstration illustrates how an agent learns to navigate a grid world using
                        Q-learning, a popular reinforcement learning algorithm. The agent's goal is to reach the green
                        cell while avoiding obstacles.</p>

                    <div class="interactive-demo">
                        <h3>Grid World Environment</h3>
                        <div class="progress-container" aria-label="Learning progress">
                            <div class="progress-bar" id="learningProgress"></div>
                        </div>
                        <div id="statusText" aria-live="polite">Agent is ready to learn. Click "Start Learning" to
                            begin.</div>

                        <div class="demo-grid" id="gridWorld"
                            aria-label="Grid world environment for reinforcement learning demo">
                            <!-- Grid cells will be generated by JavaScript -->
                        </div>

                        <div class="demo-controls">
                            <button class="btn" id="startLearningBtn">Start Learning</button>
                            <button class="btn" id="resetBtn">Reset Environment</button>
                            <button class="btn btn-secondary" id="showPolicyBtn">Show Learned Policy</button>
                            <button class="btn btn-secondary" id="showHeatmapBtn">Show Value Heatmap</button>
                            <button class="btn" id="stepByStepBtn">Step-by-Step</button>
                            <button class="btn" id="adjustParamsBtn">Adjust Parameters</button>
                        </div>

                        <div id="paramControls" class="param-controls hidden">
                            <h4>Learning Parameters</h4>
                            <div class="param-grid">
                                <div class="param-item">
                                    <div class="param-label">
                                        <label for="learningRate">Learning Rate (Œ±):</label>
                                        <span id="learningRateValue" class="param-value">0.1</span>
                                    </div>
                                    <input type="range" id="learningRate" class="param-slider" min="0.01" max="1"
                                        step="0.01" value="0.1">
                                </div>
                                <div class="param-item">
                                    <div class="param-label">
                                        <label for="discountFactor">Discount Factor (Œ≥):</label>
                                        <span id="discountFactorValue" class="param-value">0.9</span>
                                    </div>
                                    <input type="range" id="discountFactor" class="param-slider" min="0.1" max="0.99"
                                        step="0.01" value="0.9">
                                </div>
                                <div class="param-item">
                                    <div class="param-label">
                                        <label for="explorationRate">Exploration Rate (Œµ):</label>
                                        <span id="explorationRateValue" class="param-value">0.3</span>
                                    </div>
                                    <input type="range" id="explorationRate" class="param-slider" min="0.01" max="1"
                                        step="0.01" value="0.3">
                                </div>
                                <div class="param-item">
                                    <div class="param-label">
                                        <label for="episodes">Number of Episodes:</label>
                                        <span id="episodesValue" class="param-value">100</span>
                                    </div>
                                    <input type="range" id="episodes" class="param-slider" min="10" max="500" step="10"
                                        value="100">
                                </div>
                            </div>
                            <div style="margin-top: 1rem; text-align: center;">
                                <p class="info-box" style="margin: 0; font-size: 0.9rem;">
                                    Changes to parameters take effect immediately on the next learning iteration.
                                </p>
                            </div>
                        </div>
                    </div>

                    <h3>How This Demo Works</h3>
                    <p>The agent uses Q-learning, which maintains a table of Q-values for each state-action pair. These
                        Q-values represent the expected future reward for taking a particular action in a particular
                        state.</p>

                    <p>The update rule for Q-learning is:</p>
                    <div class="info-box">
                        <p><strong>Q(s,a) ‚Üê Q(s,a) + Œ±[r + Œ≥¬∑max<sub>a'</sub>Q(s',a') - Q(s,a)]</strong></p>
                        <p>Where:</p>
                        <ul>
                            <li><strong>Œ±</strong> is the learning rate (how quickly the agent incorporates new
                                information)</li>
                            <li><strong>r</strong> is the immediate reward</li>
                            <li><strong>Œ≥</strong> is the discount factor (how much future rewards are valued)</li>
                            <li><strong>s'</strong> is the next state</li>
                            <li><strong>a'</strong> is the next action</li>
                        </ul>
                        <p>In each iteration, the agent:</p>
                        <ol>
                            <li>Observes its current state</li>
                            <li>Chooses an action (sometimes exploring, sometimes exploiting)</li>
                            <li>Receives a reward and observes the new state</li>
                            <li>Updates its Q-values based on this experience</li>
                            <li>Moves to the new state and repeats</li>
                        </ol>
                    </div>

                    <h3>Understanding the Visualization</h3>

                    <div class="info-box">
                        <p><strong>Elements in the grid:</strong></p>
                        <ul>
                            <li>ü§ñ - The agent trying to learn the optimal path</li>
                            <li>G (Green cell) - The goal state with high reward</li>
                            <li>X (Dark cell) - Obstacles that should be avoided</li>
                            <li>Arrows (when showing policy) - The learned best direction to move in each state</li>
                            <li>Numbers (when enabled) - The Q-values of the best actions in each state</li>
                        </ul>
                        <p><strong>Controls:</strong></p>
                        <ul>
                            <li><strong>Start Learning</strong> - Begin the learning process</li>
                            <li><strong>Reset Environment</strong> - Clear all learned values and start fresh</li>
                            <li><strong>Show Learned Policy</strong> - Display the best actions the agent has learned
                            </li>
                            <li><strong>Step-by-Step</strong> - Toggle between continuous learning and step-by-step mode
                            </li>
                            <li><strong>Adjust Parameters</strong> - Modify learning rate, discount factor, exploration
                                rate, and number of episodes</li>
                        </ul>
                    </div>
                </div>
            </section>

            <section class="card" id="resources">
                <div class="card-header">
                    <h2>Further Learning Resources</h2>
                </div>
                <div class="card-body">
                    <h3>Books</h3>
                    <ul>
                        <li>"Reinforcement Learning: An Introduction" by Richard S. Sutton and Andrew G. Barto</li>
                        <li>"Deep Reinforcement Learning Hands-On" by Maxim Lapan</li>
                        <li>"Algorithms for Reinforcement Learning" by Csaba Szepesv√°ri</li>
                        <li>"Deep Learning" by Ian Goodfellow, Yoshua Bengio, and Aaron Courville (Chapter 16)</li>
                        <li>"Artificial Intelligence: A Modern Approach" by Stuart Russell and Peter Norvig (Chapters on
                            MDPs and RL)</li>
                    </ul>

                    <h3>Online Courses</h3>
                    <ul>
                        <li>Stanford University's "CS234: Reinforcement Learning"</li>
                        <li>University of Alberta's "Reinforcement Learning Specialization" on Coursera</li>
                        <li>DeepMind's "Introduction to Reinforcement Learning" with David Silver</li>
                        <li>Berkeley's "Deep Reinforcement Learning" course (CS285)</li>
                        <li>OpenAI's "Spinning Up in Deep RL"</li>
                    </ul>

                    <h3>Libraries and Frameworks</h3>
                    <ul>
                        <li>OpenAI Gym: A toolkit for developing and comparing RL algorithms</li>
                        <li>Stable Baselines3: A set of improved implementations of RL algorithms</li>
                        <li>TensorFlow-Agents: A library for RL in TensorFlow</li>
                        <li>PyTorch RL: RL implementations in PyTorch</li>
                        <li>RLlib: A scalable reinforcement learning library</li>
                        <li>Dopamine: A research framework for fast prototyping of RL algorithms</li>
                    </ul>

                    <h3>Research Papers</h3>
                    <ul>
                        <li>"Playing Atari with Deep Reinforcement Learning" (Mnih et al., 2013)</li>
                        <li>"Human-level control through deep reinforcement learning" (Mnih et al., 2015)</li>
                        <li>"Mastering the game of Go with deep neural networks and tree search" (Silver et al., 2016)
                        </li>
                        <li>"Proximal Policy Optimization Algorithms" (Schulman et al., 2017)</li>
                        <li>"Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm"
                            (Silver et al., 2017)</li>
                        <li>"Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic
                            Actor" (Haarnoja et al., 2018)</li>
                    </ul>

                    <div class="info-box">
                        <p><strong>Getting Started with Practical RL:</strong></p>
                        <p>If you're new to reinforcement learning and want to start experimenting:</p>
                        <ol>
                            <li>Install Python and necessary libraries (PyTorch/TensorFlow, Gym)</li>
                            <li>Begin with simple environments like CartPole or MountainCar in OpenAI Gym</li>
                            <li>Implement basic algorithms like Q-learning or SARSA</li>
                            <li>Gradually move to more complex environments and algorithms</li>
                            <li>Join communities like r/reinforcementlearning or the RL Discord for help and discussion
                            </li>
                        </ol>
                    </div>

                    <h3>Learning Path for Beginners</h3>

                    <div class="algorithm-comparison">
                        <div class="card">
                            <div class="card-header">
                                <h3>Stage 1: Foundations</h3>
                            </div>
                            <div class="card-body">
                                <ol>
                                    <li>Learn basic machine learning concepts</li>
                                    <li>Study Markov decision processes</li>
                                    <li>Understand value functions and policies</li>
                                    <li>Implement basic tabular methods (Q-learning)</li>
                                    <li>Experiment with simple environments</li>
                                </ol>
                                <p><strong>Time estimate:</strong> 4-6 weeks</p>
                            </div>
                        </div>

                        <div class="card">
                            <div class="card-header">
                                <h3>Stage 2: Deep RL</h3>
                            </div>
                            <div class="card-body">
                                <ol>
                                    <li>Learn deep learning fundamentals</li>
                                    <li>Study function approximation in RL</li>
                                    <li>Implement DQN and its variants</li>
                                    <li>Understand policy gradients</li>
                                    <li>Explore actor-critic methods</li>
                                </ol>
                                <p><strong>Time estimate:</strong> 8-12 weeks</p>
                            </div>
                        </div>

                        <div class="card">
                            <div class="card-header">
                                <h3>Stage 3: Advanced Topics</h3>
                            </div>
                            <div class="card-body">
                                <ol>
                                    <li>Study exploration strategies</li>
                                    <li>Learn model-based RL</li>
                                    <li>Explore multi-agent RL</li>
                                    <li>Understand meta-learning and transfer</li>
                                    <li>Apply RL to real-world problems</li>
                                </ol>
                                <p><strong>Time estimate:</strong> 12+ weeks</p>
                            </div>
                        </div>
                    </div>
                </div>
            </section>
        </main>
    </div>

    <!-- Settings Panel -->
    <div id="settingsPanel" class="settings-panel">
        <div class="settings-header">
            <h3>Settings</h3>
            <button id="closeSettings" class="settings-close" aria-label="Close settings">&times;</button>
        </div>

        <div class="settings-section">
            <h4 class="settings-title">Appearance</h4>
            <div class="settings-option flex-between">
                <label class="settings-label">Dark Mode</label>
                <label class="switch">
                    <input type="checkbox" id="darkModeToggle">
                    <span class="slider"></span>
                </label>
            </div>
            <div class="settings-option">
                <label class="settings-label" for="fontSize">Font Size</label>
                <input type="range" id="fontSize" class="settings-slider" min="12" max="24" value="16">
            </div>
        </div>

        <div class="settings-section">
            <h4 class="settings-title">Simulation</h4>
            <div class="settings-option">
                <label class="settings-label" for="simulationSpeed">Simulation Speed: <span
                        id="simulationSpeedValueDisplay">50</span></label>
                <input type="range" id="simulationSpeed" class="settings-slider" min="10" max="500" value="50">
            </div>
            <div class="settings-option flex-between">
                <label class="settings-label">Show Q-Values</label>
                <label class="switch">
                    <input type="checkbox" id="showQValues">
                    <span class="slider"></span>
                </label>
            </div>
        </div>

        <div class="settings-section">
            <h4 class="settings-title">Accessibility</h4>
            <div class="settings-option flex-between">
                <label class="settings-label">High Contrast</label>
                <label class="switch">
                    <input type="checkbox" id="highContrast">
                    <span class="slider"></span>
                </label>
            </div>
            <div class="settings-option flex-between">
                <label class="settings-label">Reduced Motion</label>
                <label class="switch">
                    <input type="checkbox" id="reducedMotion">
                    <span class="slider"></span>
                </label>
            </div>
        </div>

        <div class="settings-section">
            <h4 class="settings-title">Content Display</h4>
            <div class="settings-option flex-between">
                <label class="settings-label">Show Definitions</label>
                <label class="switch">
                    <input type="checkbox" id="showDefinitions" checked>
                    <span class="slider"></span>
                </label>
            </div>
            <div class="settings-option flex-between">
                <label class="settings-label">Show Advanced Content</label>
                <label class="switch">
                    <input type="checkbox" id="showAdvanced">
                    <span class="slider"></span>
                </label>
            </div>
        </div>
    </div>

    <!-- Floating Action Button and Chat Assistant -->
    <div class="fab" id="chatFab" aria-label="Open chat assistant">
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none"
            stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
            <path d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path>
        </svg>
    </div>

    <div class="chat-container" id="chatContainer">
        <div class="chat-header">
            <h3>RL Assistant</h3>
            <button class="chat-close" id="chatClose" aria-label="Close chat">&times;</button>
        </div>
        <div class="chat-messages" id="chatMessages">
            <div class="chat-message bot">
                Hello! I'm your RL assistant. Ask me any questions about reinforcement learning.
            </div>
        </div>
        <div class="chat-input-container">
            <textarea class="chat-input" id="chatInput" placeholder="Type your question..." rows="1"
                aria-label="Type your message"></textarea>
            <button class="chat-send" id="chatSend" aria-label="Send message">
                <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none"
                    stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                    <line x1="22" y1="2" x2="11" y2="13"></line>
                    <polygon points="22 2 15 22 11 13 2 9 22 2"></polygon>
                </svg>
            </button>
        </div>
    </div>

    <!-- Tour overlay -->
    <div id="tourOverlay"
        style="display: none; position: fixed; top: 0; left: 0; right: 0; bottom: 0; background: rgba(0,0,0,0.5); z-index: 1000;">
        <div id="tourTooltip"
            style="position: absolute; background: white; border-radius: 8px; padding: 1rem; max-width: 300px; box-shadow: 0 4px 12px rgba(0,0,0,0.15);">
            <h4 id="tourTitle" style="margin-top: 0; color: var(--primary);">Welcome to the Tour</h4>
            <p id="tourContent">This interactive tour will show you the key features of this Reinforcement Learning
                page.</p>
            <div style="display: flex; justify-content: space-between; margin-top: 1rem;">
                <button id="tourPrev" class="btn" style="background-color: var(--text-light);">Previous</button>
                <button id="tourNext" class="btn">Next</button>
            </div>
            <button id="tourClose"
                style="position: absolute; top: 0.5rem; right: 0.5rem; background: none; border: none; font-size: 1.2rem; cursor: pointer;">&times;</button>
        </div>
    </div>

    <script>
        // Global state variables for settings
        let currentShowQValuesSetting = false;
        let currentSimulationSpeed = 50;

        // Global instances for major components
        let gridWorldDemo;
        let guidedTour;

        // Tab functionality
        document.querySelectorAll('.tab').forEach(tab => {
            tab.addEventListener('click', () => {
                document.querySelectorAll('.tab').forEach(t => {
                    t.classList.remove('active');
                    t.setAttribute('aria-selected', 'false');
                    t.setAttribute('tabindex', '-1');
                });
                tab.classList.add('active');
                tab.setAttribute('aria-selected', 'true');
                tab.setAttribute('tabindex', '0');
                document.querySelectorAll('.tab-content').forEach(content => content.classList.remove('active'));
                const tabContent = document.getElementById(tab.dataset.tab);
                if (tabContent) tabContent.classList.add('active');
            });
            tab.addEventListener('keydown', (e) => {
                const tabs = Array.from(document.querySelectorAll('.tab'));
                const index = tabs.indexOf(tab);
                if (e.key === 'ArrowRight') {
                    e.preventDefault();
                    const nextTab = tabs[(index + 1) % tabs.length];
                    nextTab.click();
                    nextTab.focus();
                } else if (e.key === 'ArrowLeft') {
                    e.preventDefault();
                    const prevTab = tabs[(index - 1 + tabs.length) % tabs.length];
                    prevTab.click();
                    prevTab.focus();
                }
            });
        });

        // Smooth scrolling for navigation links
        document.querySelectorAll('.sidebar-nav a').forEach(link => {
            link.addEventListener('click', function (e) {
                e.preventDefault();
                document.querySelectorAll('.sidebar-nav a').forEach(l => {
                    l.classList.remove('active');
                    l.removeAttribute('aria-current');
                });
                this.classList.add('active');
                this.setAttribute('aria-current', 'page');
                const targetId = this.getAttribute('href').substring(1);
                const targetElement = document.getElementById(targetId);
                if (targetElement) {
                    window.scrollTo({
                        top: targetElement.offsetTop - 80, // Adjusted for fixed header
                        behavior: 'smooth'
                    });
                }
            });
        });

        // Theme toggle functionality
        const themeToggle = document.getElementById('themeToggle');
        const themeText = document.getElementById('themeText');
        const darkModeToggle = document.getElementById('darkModeToggle');

        function toggleDarkMode() {
            document.body.classList.toggle('dark-mode');
            const isDarkMode = document.body.classList.contains('dark-mode');
            if (themeText) themeText.textContent = isDarkMode ? 'Light Mode' : 'Dark Mode';
            if (darkModeToggle) darkModeToggle.checked = isDarkMode;
            localStorage.setItem('darkMode', isDarkMode);
        }

        if (themeToggle) themeToggle.addEventListener('click', toggleDarkMode);
        if (darkModeToggle) darkModeToggle.addEventListener('change', toggleDarkMode);
        if (localStorage.getItem('darkMode') === 'true') {
            toggleDarkMode();
        }

        // Settings panel functionality
        const settingsBtn = document.getElementById('settingsBtn');
        const settingsPanel = document.getElementById('settingsPanel');
        const closeSettings = document.getElementById('closeSettings');

        if (settingsBtn) settingsBtn.addEventListener('click', () => settingsPanel?.classList.add('open'));
        if (closeSettings) closeSettings.addEventListener('click', () => settingsPanel?.classList.remove('open'));
        document.addEventListener('click', (e) => {
            if (settingsPanel && !settingsPanel.contains(e.target) && e.target !== settingsBtn) {
                settingsPanel.classList.remove('open');
            }
        });

        // Font size adjustment
        const fontSizeSlider = document.getElementById('fontSize');
        if (fontSizeSlider) {
            fontSizeSlider.addEventListener('input', () => {
                document.documentElement.style.fontSize = `${fontSizeSlider.value}px`;
                localStorage.setItem('fontSize', fontSizeSlider.value);
            });
            if (localStorage.getItem('fontSize')) {
                const savedSize = localStorage.getItem('fontSize');
                fontSizeSlider.value = savedSize;
                document.documentElement.style.fontSize = `${savedSize}px`;
            }
        }

        // Simulation speed control
        const simulationSpeedSlider = document.getElementById('simulationSpeed');
        if (simulationSpeedSlider) {
            simulationSpeedSlider.addEventListener('input', () => {
                currentSimulationSpeed = parseInt(simulationSpeedSlider.value);
                localStorage.setItem('simulationSpeed', currentSimulationSpeed);
                if (gridWorldDemo) {
                    gridWorldDemo.setSimulationSpeed(currentSimulationSpeed);
                }
                const speedValueDisplay = document.getElementById('simulationSpeedValue');
                if (speedValueDisplay) speedValueDisplay.textContent = currentSimulationSpeed;
            });
            if (localStorage.getItem('simulationSpeed')) {
                currentSimulationSpeed = parseInt(localStorage.getItem('simulationSpeed'));
                simulationSpeedSlider.value = currentSimulationSpeed;
            }
            const initialSpeedValueDisplay = document.getElementById('simulationSpeedValue');
            if (initialSpeedValueDisplay) initialSpeedValueDisplay.textContent = currentSimulationSpeed;
        }

        // High contrast mode
        const highContrastToggle = document.getElementById('highContrast');
        if (highContrastToggle) {
            highContrastToggle.addEventListener('change', () => {
                document.body.classList.toggle('high-contrast', highContrastToggle.checked);
                localStorage.setItem('highContrast', highContrastToggle.checked);
            });
            if (localStorage.getItem('highContrast') === 'true') {
                highContrastToggle.checked = true;
                document.body.classList.add('high-contrast');
            }
        }

        // Reduced motion
        const reducedMotionToggle = document.getElementById('reducedMotion');
        if (reducedMotionToggle) {
            reducedMotionToggle.addEventListener('change', () => {
                document.body.classList.toggle('reduced-motion', reducedMotionToggle.checked);
                localStorage.setItem('reducedMotion', reducedMotionToggle.checked);
            });
            if (localStorage.getItem('reducedMotion') === 'true') {
                reducedMotionToggle.checked = true;
                document.body.classList.add('reduced-motion');
            }
        }

        // Show Q-values toggle
        const showQValuesToggle = document.getElementById('showQValues');
        if (showQValuesToggle) {
            showQValuesToggle.addEventListener('change', () => {
                currentShowQValuesSetting = showQValuesToggle.checked;
                localStorage.setItem('showQValues', currentShowQValuesSetting);
                if (gridWorldDemo) {
                    gridWorldDemo.showQValues = currentShowQValuesSetting;
                    if (gridWorldDemo.policyShown) {
                        gridWorldDemo.showPolicy();
                    }
                }
            });
            if (localStorage.getItem('showQValues') === 'true') {
                showQValuesToggle.checked = true;
                currentShowQValuesSetting = true;
            }
        }

        // Show definitions toggle
        const showDefinitionsToggle = document.getElementById('showDefinitions');
        if (showDefinitionsToggle) {
            showDefinitionsToggle.addEventListener('change', () => {
                const glossaryTerms = document.querySelectorAll('.glossary-term');
                if (!showDefinitionsToggle.checked) {
                    glossaryTerms.forEach(term => {
                        term.style.borderBottom = 'none';
                        term.style.cursor = 'default';
                        term.dataset.originalDefinition = term.dataset.definition;
                        term.removeAttribute('data-definition');
                    });
                } else {
                    glossaryTerms.forEach(term => {
                        term.style.borderBottom = '';
                        term.style.cursor = '';
                        if (term.dataset.originalDefinition) {
                            term.dataset.definition = term.dataset.originalDefinition;
                        }
                    });
                }
                localStorage.setItem('showDefinitions', showDefinitionsToggle.checked);
            });
            if (localStorage.getItem('showDefinitions') === 'false') {
                showDefinitionsToggle.checked = false;
                showDefinitionsToggle.dispatchEvent(new Event('change'));
            }
        }

        // Show advanced content toggle
        const showAdvancedToggle = document.getElementById('showAdvanced');
        if (showAdvancedToggle) {
            const advancedElements = document.querySelectorAll('.info-box, .pseudocode, .code-snippet');
            showAdvancedToggle.addEventListener('change', () => {
                const display = showAdvancedToggle.checked ? 'block' : 'none';
                advancedElements.forEach(el => {
                    if (el.classList.contains('info-box') || el.classList.contains('pseudocode') || el.classList.contains('code-snippet')) {
                        el.style.display = display;
                    }
                });
                localStorage.setItem('showAdvanced', showAdvancedToggle.checked);
            });
            if (localStorage.getItem('showAdvanced') === 'true') {
                showAdvancedToggle.checked = true;
            } else if (localStorage.getItem('showAdvanced') === 'false') {
                showAdvancedToggle.checked = false;
                showAdvancedToggle.dispatchEvent(new Event('change'));
            }
        }

        // Enhanced keyboard navigation
        document.addEventListener('keydown', function (e) {
            const settingsOpen = settingsPanel && settingsPanel.classList.contains('open');
            // Assuming chatContainer is defined in app_features.js and might not be directly accessible here
            // For safety, check its existence or rely on an event/state if inter-file communication is needed for chatOpen status
            // For now, let's assume chatContainer might not be available here directly for this check.
            // const chatOpen = document.getElementById('chatContainer') && document.getElementById('chatContainer').classList.contains('active');
            let chatOpen = false; // Placeholder
            const chatContainerElement = document.getElementById('chatContainer');
            if (chatContainerElement) {
                chatOpen = chatContainerElement.classList.contains('active');
            }

            const modalOpen = settingsOpen || chatOpen;

            if (document.activeElement.tagName === 'INPUT' || document.activeElement.tagName === 'TEXTAREA') {
                return;
            }

            if (!modalOpen) {
                if (e.key === '/' || (e.key === 'f' && (e.ctrlKey || e.metaKey))) {
                    e.preventDefault();
                    if (searchInput) searchInput.focus(); // searchInput is in app_features.js, ensure it's globally accessible or loaded
                } else if (e.key === 'Escape') {
                    if (document.activeElement && typeof document.activeElement.blur === 'function') {
                        document.activeElement.blur();
                    }
                } else if (e.key === 's' && !chatOpen) {
                    e.preventDefault();
                    if (settingsBtn) settingsBtn.click();
                } else if (e.key === 'c') {
                    e.preventDefault();
                    const chatFabElement = document.getElementById('chatFab'); // chatFab is in app_features.js
                    if (chatFabElement) chatFabElement.click();
                } else if (e.key === 'd') {
                    e.preventDefault();
                    if (themeToggle) themeToggle.click();
                }
            } else {
                if (e.key === 'Escape') {
                    if (settingsOpen && closeSettings) {
                        closeSettings.click();
                    }
                    if (chatOpen) {
                        const chatCloseElement = document.getElementById('chatClose'); // chatClose is in app_features.js
                        if (chatCloseElement) chatCloseElement.click();
                    }
                }
            }
        });

        // Add keyboard shortcut hints
        function addKeyboardShortcuts() {
            const shortcutsInfo = document.createElement('div');
            shortcutsInfo.className = 'info-box';
            shortcutsInfo.style.marginTop = '1rem';
            shortcutsInfo.style.fontSize = '0.85rem';
            shortcutsInfo.innerHTML = `
        <h4>Keyboard Shortcuts</h4>
        <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(150px, 1fr)); gap: 0.5rem; margin-top: 0.5rem;">
            <div><kbd>/</kbd> or <kbd>Ctrl/‚åò</kbd>+<kbd>F</kbd> - Search</div>
            <div><kbd>s</kbd> - Settings</div>
            <div><kbd>c</kbd> - Chat</div>
            <div><kbd>d</kbd> - Toggle Dark Mode</div>
            <div><kbd>Esc</kbd> - Close/Cancel</div>
        </div>
    `;
            const style = document.createElement('style');
            style.textContent = `
        kbd {
            background-color: var(--background-alt);
            border: 1px solid var(--border);
            border-radius: 3px;
            box-shadow: 0 1px 1px var(--shadow);
            color: var(--text);
            display: inline-block;
            font-size: 0.85em;
            font-family: monospace;
            line-height: 1;
            padding: 2px 4px;
            white-space: nowrap;
        }
    `;
            document.head.appendChild(style);
            const sidebar = document.querySelector('.sidebar');
            if (sidebar) sidebar.appendChild(shortcutsInfo);
        }
        addKeyboardShortcuts();

        // Add an accessible "Skip to navigation" link & other ARIA enhancements
        function enhanceAccessibility() {
            const skipToNavLink = document.createElement('a');
            skipToNavLink.href = '#main-navigation';
            skipToNavLink.className = 'skip-link'; // Ensure this class is styled to be visually hidden until focused
            skipToNavLink.textContent = 'Skip to navigation';
            document.body.insertBefore(skipToNavLink, document.body.firstChild);

            const sidebarNav = document.querySelector('.sidebar-nav');
            if (sidebarNav) {
                sidebarNav.id = 'main-navigation';
                sidebarNav.setAttribute('role', 'navigation');
                sidebarNav.setAttribute('aria-label', 'Main navigation');
            }

            const searchInputElement = document.getElementById('searchInput'); // searchInput from app_features.js
            if (searchInputElement) searchInputElement.setAttribute('aria-label', 'Search content');

            const focusStyle = document.createElement('style');
            focusStyle.textContent = `
        a:focus-visible, button:focus-visible, input:focus-visible, textarea:focus-visible, [tabindex="0"]:focus-visible {
            outline: 2px solid var(--primary);
            outline-offset: 2px;
            box-shadow: 0 0 0 4px rgba(var(--primary-rgb), 0.3); /* Ensure --primary-rgb is defined or use a fixed color */
        }
        .search-highlight:focus {
            background-color: rgba(59, 130, 246, 0.5);
            outline: 2px solid var(--primary);
        }
        .skip-link {
            position: absolute;
            top: -40px;
            left: 0;
            background: var(--background);
            color: var(--text);
            padding: 10px;
            z-index: 100000; /* Ensure it is on top */
            transition: top 0.3s;
        }
        .skip-link:focus {
            top: 0;
        }
    `;
            document.head.appendChild(focusStyle);
        }
        enhanceAccessibility();

        // Add scroll tracking to highlight current section in navigation
        function updateActiveNavOnScroll() {
            const sections = document.querySelectorAll('section.card');
            const navLinks = document.querySelectorAll('.sidebar-nav a');
            const scrollPosition = window.scrollY + 100;

            sections.forEach(section => {
                const sectionTop = section.offsetTop;
                const sectionHeight = section.offsetHeight;
                const sectionId = section.id;

                if (scrollPosition >= sectionTop && scrollPosition < sectionTop + sectionHeight) {
                    navLinks.forEach(link => {
                        link.classList.remove('active');
                        link.removeAttribute('aria-current');
                    });
                    const activeLink = document.querySelector(`.sidebar-nav a[href="#${sectionId}"]`);
                    if (activeLink) {
                        activeLink.classList.add('active');
                        activeLink.setAttribute('aria-current', 'page');
                    }
                }
            });
        }
        window.addEventListener('scroll', updateActiveNavOnScroll);

        // DOMContentLoaded: Initialize demos, tours, and specific event listeners
        window.addEventListener('DOMContentLoaded', () => {
            updateActiveNavOnScroll(); // Initial call for scroll tracking

            // Initialize GridWorldDemo (GridWorldDemo class should be loaded from gridworld.js)
            if (typeof GridWorldDemo !== 'undefined') {
                gridWorldDemo = new GridWorldDemo(currentShowQValuesSetting, currentSimulationSpeed);
            } else {
                console.error('GridWorldDemo class not found. Ensure gridworld.js is loaded.');
            }

            // Add keyboard navigation for grid (dependent on gridWorldDemo)
            document.addEventListener('keydown', (e) => {
                if (gridWorldDemo && !gridWorldDemo.isLearning) {
                    let newX = gridWorldDemo.agentPos.x;
                    let newY = gridWorldDemo.agentPos.y;
                    switch (e.key) {
                        case 'ArrowUp': newY = Math.max(0, newY - 1); break;
                        case 'ArrowRight': newX = Math.min(gridWorldDemo.gridSize - 1, newX + 1); break;
                        case 'ArrowDown': newY = Math.min(gridWorldDemo.gridSize - 1, newY + 1); break;
                        case 'ArrowLeft': newX = Math.max(0, newX - 1); break;
                        default: return;
                    }
                    const isObstacle = gridWorldDemo.obstacles.some(obs => obs.x === newX && obs.y === newY);
                    if (!isObstacle) {
                        gridWorldDemo.updateAgentPosition(newX, newY);
                        if (newX === gridWorldDemo.goalPos.x && newY === gridWorldDemo.goalPos.y) {
                            const statusText = document.getElementById('statusText');
                            if (statusText) statusText.textContent = 'Goal reached! üéâ';
                        }
                    }
                }
            });

            // Handle step button clicks in step mode (dependent on gridWorldDemo)
            const stepByStepBtn = document.getElementById('stepByStepBtn');
            if (stepByStepBtn) {
                stepByStepBtn.addEventListener('click', () => {
                    if (gridWorldDemo && gridWorldDemo.stepMode && gridWorldDemo.isLearning) {
                        gridWorldDemo.showEpisodeStep();
                    }
                });
            }

            // Initialize GuidedTour (GuidedTour class should be loaded from app_features.js)
            setTimeout(() => {
                if (typeof GuidedTour !== 'undefined') {
                    guidedTour = new GuidedTour();
                    if (!localStorage.getItem('tourShown')) {
                        localStorage.setItem('tourShown', 'true');
                        if (guidedTour && typeof guidedTour.startTour === 'function') {
                            guidedTour.startTour();
                        }
                    }
                } else {
                    console.error('GuidedTour class not found. Ensure app_features.js is loaded.');
                }
            }, 1000);
        });

        class GridWorldDemo {
            constructor(initialShowQValuesSetting, initialSimulationSpeed) {
                this.gridSize = 5;
                this.grid = [];
                this.agentPos = { x: 0, y: 0 };
                this.goalPos = { x: 4, y: 4 };
                this.obstacles = [
                    { x: 1, y: 1 },
                    { x: 2, y: 3 },
                    { x: 3, y: 2 }
                ];
                this.qTable = {}; // State-action values
                this.learningRate = 0.1;
                this.discountFactor = 0.9;
                this.explorationRate = 0.3;
                this.maxEpisodes = 100;
                this.currentEpisode = 0;
                this.isLearning = false;
                this.stepMode = false;
                this.currentStep = 0;
                this.episodeHistory = [];
                this.showQValues = initialShowQValuesSetting;
                this.simulationSpeed = initialSimulationSpeed;
                this.policyShown = false;
                this.stats = {
                    totalRewards: 0,
                    avgRewardsPerEpisode: 0,
                    avgStepsPerEpisode: 0,
                    successRate: 0,
                    episodesCompleted: 0
                };
                this.episodeStats = [];
                this.heatmapMode = false;
                this.paramControlsDisplayed = false;

                this.initGrid();
                this.setupEventListeners();
                this.updateParameterLabels();
            }

            initGrid() {
                const gridContainer = document.getElementById('gridWorld');
                gridContainer.innerHTML = '';

                // Create grid
                for (let y = 0; y < this.gridSize; y++) {
                    this.grid[y] = [];
                    for (let x = 0; x < this.gridSize; x++) {
                        const cell = document.createElement('div');
                        cell.className = 'grid-cell';
                        cell.id = `cell-${x}-${y}`;
                        cell.setAttribute('data-x', x);
                        cell.setAttribute('data-y', y);

                        // Check if cell is goal
                        if (x === this.goalPos.x && y === this.goalPos.y) {
                            cell.classList.add('goal');
                            cell.textContent = 'G';
                            cell.setAttribute('aria-label', 'Goal');
                        }

                        // Check if cell is obstacle
                        const isObstacle = this.obstacles.some(obs => obs.x === x && obs.y === y);
                        if (isObstacle) {
                            cell.classList.add('obstacle');
                            cell.textContent = 'X';
                            cell.setAttribute('aria-label', 'Obstacle');
                        }

                        // Check if cell has agent
                        if (x === this.agentPos.x && y === this.agentPos.y) {
                            cell.classList.add('agent');
                            cell.setAttribute('aria-label', 'Agent');
                        }

                        // Add click event to cells for user interaction
                        cell.addEventListener('click', () => {
                            if (!this.isLearning) {
                                this.handleCellClick(x, y);
                            }
                        });

                        gridContainer.appendChild(cell);
                        this.grid[y][x] = cell;
                    }
                }

                // Initialize Q-table
                for (let y = 0; y < this.gridSize; y++) {
                    for (let x = 0; x < this.gridSize; x++) {
                        const state = `${x},${y}`;
                        this.qTable[state] = {
                            up: 0,
                            right: 0,
                            down: 0,
                            left: 0
                        };
                    }
                }

                this.policyShown = false;
                this.heatmapMode = false;
                this.episodeStats = [];
                this.resetStats();
            }

            resetStats() {
                this.stats = {
                    totalRewards: 0,
                    avgRewardsPerEpisode: 0,
                    avgStepsPerEpisode: 0,
                    successRate: 0,
                    episodesCompleted: 0
                };
                this.updateStatsDisplay();
            }

            updateStatsDisplay() {
                // Create or update stats container
                let statsContainer = document.getElementById('gridWorldStats');
                if (!statsContainer) {
                    statsContainer = document.createElement('div');
                    statsContainer.id = 'gridWorldStats';
                    statsContainer.className = 'info-box';
                    statsContainer.style.marginTop = '1rem';
                    statsContainer.style.fontSize = '0.9rem';

                    const demoContainer = document.querySelector('.interactive-demo');
                    if (demoContainer) demoContainer.appendChild(statsContainer);
                }

                // Update stats content
                statsContainer.innerHTML = `
            <h4>Learning Statistics</h4>
            <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(150px, 1fr)); gap: 0.5rem; margin-top: 0.5rem;">
                <div>Episodes Completed: <strong>${this.stats.episodesCompleted}</strong></div>
                <div>Success Rate: <strong>${(this.stats.successRate * 100).toFixed(1)}%</strong></div>
                <div>Avg. Steps/Episode: <strong>${this.stats.avgStepsPerEpisode.toFixed(1)}</strong></div>
                <div>Avg. Reward/Episode: <strong>${this.stats.avgRewardsPerEpisode.toFixed(1)}</strong></div>
            </div>
            ${this.episodeStats.length > 0 ? `
            <div style="margin-top: 0.5rem;">
                <canvas id="learningChart" width="300" height="150"></canvas>
            </div>
            ` : ''}
        `;

                // Update chart if we have data
                if (this.episodeStats.length > 0) {
                    this.updateLearningChart();
                }
            }

            updateLearningChart() {
                const canvas = document.getElementById('learningChart');
                if (!canvas) return;

                const ctx = canvas.getContext('2d');
                ctx.clearRect(0, 0, canvas.width, canvas.height);

                // Draw chart background
                ctx.fillStyle = 'rgba(243, 244, 246, 0.5)';
                ctx.fillRect(0, 0, canvas.width, canvas.height);

                // Find max and min values
                const rewards = this.episodeStats.map(stat => stat.reward);
                const steps = this.episodeStats.map(stat => stat.steps);
                const maxReward = Math.max(...rewards);
                const minReward = Math.min(...rewards);
                const maxSteps = Math.max(...steps);

                // Draw axes
                ctx.strokeStyle = '#6b7280';
                ctx.lineWidth = 1;
                ctx.beginPath();
                ctx.moveTo(40, 10);
                ctx.lineTo(40, 120);
                ctx.lineTo(290, 120);
                ctx.stroke();

                // Draw title
                ctx.fillStyle = '#1f2937';
                ctx.font = '10px sans-serif';
                ctx.fillText('Learning Progress', 120, 10);

                // Draw axis labels
                ctx.fillText('Episodes', 140, 135);
                ctx.save();
                ctx.translate(10, 70);
                ctx.rotate(-Math.PI / 2);
                ctx.fillText('Reward / Steps', 0, 0);
                ctx.restore();

                // Skip if not enough data points
                if (this.episodeStats.length < 2) return;

                // Draw reward line
                const xStep = (250) / (this.episodeStats.length - 1);
                const yRangeReward = maxReward - minReward || 1;

                ctx.strokeStyle = '#2563eb';
                ctx.lineWidth = 2;
                ctx.beginPath();
                this.episodeStats.forEach((stat, i) => {
                    const x = 40 + i * xStep;
                    const y = 120 - ((stat.reward - minReward) / yRangeReward * 100);
                    if (i === 0) ctx.moveTo(x, y);
                    else ctx.lineTo(x, y);
                });
                ctx.stroke();

                // Draw steps line
                ctx.strokeStyle = '#10b981';
                ctx.beginPath();
                this.episodeStats.forEach((stat, i) => {
                    const x = 40 + i * xStep;
                    const y = 120 - (stat.steps / maxSteps * 100);
                    if (i === 0) ctx.moveTo(x, y);
                    else ctx.lineTo(x, y);
                });
                ctx.stroke();

                // Draw legend
                ctx.fillStyle = '#2563eb';
                ctx.fillRect(200, 10, 10, 5);
                ctx.fillStyle = '#1f2937';
                ctx.fillText('Reward', 215, 15);

                ctx.fillStyle = '#10b981';
                ctx.fillRect(200, 25, 10, 5);
                ctx.fillStyle = '#1f2937';
                ctx.fillText('Steps', 215, 30);
            }

            handleCellClick(x, y) {
                // Don't allow clicking on obstacles
                const isObstacle = this.obstacles.some(obs => obs.x === x && obs.y === y);
                if (isObstacle) return;

                // Move agent to clicked cell
                this.updateAgentPosition(x, y);

                // Check if reached goal
                if (x === this.goalPos.x && y === this.goalPos.y) {
                    document.getElementById('statusText').textContent = 'Goal reached! üéâ';
                } else {
                    document.getElementById('statusText').textContent = `Agent moved to position (${x}, ${y})`;
                }
            }

            setupEventListeners() {
                document.getElementById('startLearningBtn').addEventListener('click', () => this.startLearning());
                document.getElementById('resetBtn').addEventListener('click', () => this.resetEnvironment());
                document.getElementById('showPolicyBtn').addEventListener('click', () => this.showPolicy());
                document.getElementById('stepByStepBtn').addEventListener('click', () => this.toggleStepMode());
                document.getElementById('adjustParamsBtn').addEventListener('click', () => this.toggleParamControls());
                document.getElementById('showHeatmapBtn').addEventListener('click', () => this.toggleHeatmap());

                document.getElementById('learningRate').addEventListener('input', (e) => {
                    this.learningRate = parseFloat(e.target.value);
                    document.getElementById('learningRateValue').textContent = this.learningRate.toFixed(2);
                });

                document.getElementById('discountFactor').addEventListener('input', (e) => {
                    this.discountFactor = parseFloat(e.target.value);
                    document.getElementById('discountFactorValue').textContent = this.discountFactor.toFixed(2);
                });

                document.getElementById('explorationRate').addEventListener('input', (e) => {
                    this.explorationRate = parseFloat(e.target.value);
                    document.getElementById('explorationRateValue').textContent = this.explorationRate.toFixed(2);
                });

                document.getElementById('episodes').addEventListener('input', (e) => {
                    this.maxEpisodes = parseInt(e.target.value);
                    document.getElementById('episodesValue').textContent = this.maxEpisodes;
                });
            }

            updateParameterLabels() {
                // This method might be needed if parameters are set externally initially
                // For now, assuming constructor sets initial state that matches sliders or vice-versa
                document.getElementById('learningRateValue').textContent = this.learningRate.toFixed(2);
                document.getElementById('discountFactorValue').textContent = this.discountFactor.toFixed(2);
                document.getElementById('explorationRateValue').textContent = this.explorationRate.toFixed(2);
                document.getElementById('episodesValue').textContent = this.maxEpisodes;
                //Also update slider positions
                document.getElementById('learningRate').value = this.learningRate;
                document.getElementById('discountFactor').value = this.discountFactor;
                document.getElementById('explorationRate').value = this.explorationRate;
                document.getElementById('episodes').value = this.maxEpisodes;
            }

            setSimulationSpeed(speed) {
                this.simulationSpeed = speed;
            }

            toggleHeatmap() {
                this.heatmapMode = !this.heatmapMode;

                if (this.heatmapMode) {
                    document.getElementById('showHeatmapBtn').textContent = 'Hide Value Heatmap';
                    this.showValueHeatmap();
                } else {
                    document.getElementById('showHeatmapBtn').textContent = 'Show Value Heatmap';
                    this.hideValueHeatmap();

                    if (this.policyShown) {
                        this.showPolicy();
                    }
                }
            }

            showValueHeatmap() {
                let maxValue = Number.NEGATIVE_INFINITY;
                for (let y = 0; y < this.gridSize; y++) {
                    for (let x = 0; x < this.gridSize; x++) {
                        const state = `${x},${y}`;
                        const stateValue = Math.max(...Object.values(this.qTable[state]));
                        if (stateValue > maxValue) {
                            maxValue = stateValue;
                        }
                    }
                }
                maxValue = maxValue <= 0 ? 1 : maxValue;

                for (let y = 0; y < this.gridSize; y++) {
                    for (let x = 0; x < this.gridSize; x++) {
                        const cell = this.grid[y][x];
                        if (cell.classList.contains('obstacle') || (x === this.goalPos.x && y === this.goalPos.y)) {
                            continue;
                        }
                        cell.classList.remove('agent');
                        const state = `${x},${y}`;
                        const stateValue = Math.max(...Object.values(this.qTable[state]));
                        const normalizedValue = Math.max(0, stateValue / maxValue);
                        const h = (1 - normalizedValue) * 240;
                        const s = 70;
                        const l = 50 + normalizedValue * 30;
                        cell.style.backgroundColor = `hsl(${h}, ${s}%, ${l}%)`;
                        cell.style.color = l > 60 ? '#1f2937' : 'white';
                        cell.textContent = stateValue.toFixed(1);
                    }
                }
                this.grid[this.agentPos.y][this.agentPos.x].classList.add('agent');
            }

            hideValueHeatmap() {
                for (let y = 0; y < this.gridSize; y++) {
                    for (let x = 0; x < this.gridSize; x++) {
                        const cell = this.grid[y][x];
                        if (cell.classList.contains('obstacle') || (x === this.goalPos.x && y === this.goalPos.y)) {
                            continue;
                        }
                        cell.style.backgroundColor = '';
                        cell.style.color = '';
                        cell.textContent = '';
                    }
                }
                this.grid[this.agentPos.y][this.agentPos.x].classList.add('agent');
            }

            startLearning() {
                if (this.isLearning) return;

                this.isLearning = true;
                this.currentEpisode = 0;
                this.episodeHistory = [];
                this.episodeStats = [];
                this.resetStats();

                if (this.heatmapMode) {
                    this.heatmapMode = false;
                    this.hideValueHeatmap();
                    document.getElementById('showHeatmapBtn').textContent = 'Show Value Heatmap';
                }

                document.getElementById('startLearningBtn').textContent = 'Learning...';
                document.getElementById('startLearningBtn').disabled = true;
                document.getElementById('stepByStepBtn').disabled = true;

                document.getElementById('statusText').textContent = 'Agent is learning...';

                if (this.stepMode) {
                    // Step mode: Generate all episodes first, then allow user to step through them
                    this.episodeHistory = [];
                    for (let i = 0; i < this.maxEpisodes; i++) {
                        const episodeData = this.generateEpisodeData(); // Q-table is updated here
                        this.episodeHistory.push(episodeData);
                    }
                    this.currentEpisode = 0;
                    this.currentStepInEpisodeDisplay = 0;
                    this.displayEpisodeStepByStep();

                    // Re-enable buttons as appropriate after setup
                    document.getElementById('startLearningBtn').disabled = false; // Or keep disabled if auto-starting step display
                    // stepByStepBtn will be managed by displayEpisodeStepByStep

                } else {
                    // Continuous learning mode (now with step-by-step animation within each episode)
                    let episodesProcessed = 0;
                    const runAsyncEpisodeLoop = async () => {
                        if (episodesProcessed >= this.maxEpisodes || !this.isLearning) { // Added !this.isLearning check for early exit
                            this.isLearning = false;
                            document.getElementById('startLearningBtn').textContent = 'Start Learning';
                            document.getElementById('startLearningBtn').disabled = false;
                            document.getElementById('stepByStepBtn').disabled = false;
                            document.getElementById('statusText').textContent = episodesProcessed >= this.maxEpisodes ? 'Learning complete! Click "Show Learned Policy" to see results.' : 'Learning stopped.';
                            this.updateStatsDisplay();
                            return;
                        }

                        const episodeData = await this.runEpisode();
                        this.updateEpisodeStats(episodeData, episodesProcessed);
                        const progress = ((episodesProcessed + 1) / this.maxEpisodes) * 100;
                        document.getElementById('learningProgress').style.width = `${progress}%`;
                        document.getElementById('statusText').textContent = `Learning... Episode ${episodesProcessed + 1}/${this.maxEpisodes}`;
                        this.updateStatsDisplay();

                        episodesProcessed++;
                        this.currentEpisode = episodesProcessed;

                        if (this.isLearning) { // Check again before scheduling next frame
                            requestAnimationFrame(runAsyncEpisodeLoop);
                        }
                    };
                    requestAnimationFrame(runAsyncEpisodeLoop);
                }
            }

            updateEpisodeStats(episodeData, episodeIndex) { // Added episodeIndex
                this.episodeStats.push({
                    episode: episodeIndex, // Use passed index
                    steps: episodeData.steps.length,
                    reward: episodeData.totalReward,
                    success: episodeData.success
                });

                this.stats.episodesCompleted++;
                this.stats.totalRewards += episodeData.totalReward;
                this.stats.avgRewardsPerEpisode = this.stats.totalRewards / this.stats.episodesCompleted;
                this.stats.avgStepsPerEpisode = this.episodeStats.reduce((sum, stat) => sum + stat.steps, 0) / this.stats.episodesCompleted;
                this.stats.successRate = this.episodeStats.filter(stat => stat.success).length / this.stats.episodesCompleted;
            }

            generateEpisodeData() {
                let agentX = 0;
                let agentY = 0;
                let done = false;
                let maxSteps = 100;
                let steps = 0;
                let totalReward = 0;
                let stepHistory = [];
                let success = false;

                while (!done && steps < maxSteps) {
                    const state = `${agentX},${agentY}`;
                    const action = this.selectAction(state);
                    const [nextX, nextY] = this.getNextPosition(action, agentX, agentY);
                    const reward = this.getReward(nextX, nextY);
                    totalReward += reward;

                    const nextState = `${nextX},${nextY}`;
                    const maxNextQ = Math.max(...Object.values(this.qTable[nextState]));

                    this.qTable[state][action] += this.learningRate * (
                        reward + this.discountFactor * maxNextQ - this.qTable[state][action]
                    );

                    stepHistory.push({
                        x: nextX,
                        y: nextY,
                        action: action,
                        reward: reward
                    });

                    agentX = nextX;
                    agentY = nextY;

                    if (nextX === this.goalPos.x && nextY === this.goalPos.y) {
                        done = true;
                        success = true;
                    }
                    steps++;
                }
                return { steps: stepHistory, totalReward: totalReward, success: success };
            }

            async runEpisode() { // Make it an async function
                // Agent's position for the current episode, distinct from this.agentPos for global UI update
                let episodeAgentX = 0;
                let episodeAgentY = 0;

                // Initial UI update for the start of the episode
                this.updateAgentPosition(episodeAgentX, episodeAgentY); // Sets this.agentPos

                let done = false;
                let maxStepsPerEpisode = 100;
                let currentStepsInEpisode = 0;
                let totalReward = 0;
                let stepHistory = [];
                let success = false;

                while (!done && currentStepsInEpisode < maxStepsPerEpisode && this.isLearning) { // Added this.isLearning check
                    const state = `${episodeAgentX},${episodeAgentY}`;
                    const action = this.selectAction(state);
                    const [nextX, nextY] = this.getNextPosition(action, episodeAgentX, episodeAgentY);
                    const reward = this.getReward(nextX, nextY);
                    totalReward += reward;

                    const nextState = `${nextX},${nextY}`;
                    const maxNextQ = Math.max(...Object.values(this.qTable[nextState]));

                    this.qTable[state][action] += this.learningRate * (
                        reward + this.discountFactor * maxNextQ - this.qTable[state][action]
                    );

                    stepHistory.push({ x: nextX, y: nextY, action: action, reward: reward });

                    episodeAgentX = nextX;
                    episodeAgentY = nextY;

                    this.updateAgentPosition(episodeAgentX, episodeAgentY);

                    if (episodeAgentX === this.goalPos.x && episodeAgentY === this.goalPos.y) {
                        done = true;
                        success = true;
                    }
                    currentStepsInEpisode++;

                    if (!this.stepMode && this.isLearning) {
                        // Invert speed: higher slider value means faster (shorter delay)
                        const sliderMin = 10;
                        const sliderMax = 500;
                        let actualDelay = (sliderMax + sliderMin) - this.simulationSpeed;
                        actualDelay = Math.max(1, actualDelay); // Ensure minimum 1ms delay
                        await new Promise(resolve => setTimeout(resolve, actualDelay));
                    }
                }
                return { steps: stepHistory, totalReward: totalReward, success: success };
            }

            selectAction(state) {
                if (Math.random() < this.explorationRate) {
                    const actions = ['up', 'right', 'down', 'left'];
                    return actions[Math.floor(Math.random() * actions.length)];
                } else {
                    const qValues = this.qTable[state];
                    return Object.keys(qValues).reduce((a, b) => qValues[a] > qValues[b] ? a : b);
                }
            }

            getNextPosition(action, x, y) {
                let nextX = x;
                let nextY = y;

                switch (action) {
                    case 'up': nextY = Math.max(0, nextY - 1); break;
                    case 'right': nextX = Math.min(this.gridSize - 1, nextX + 1); break;
                    case 'down': nextY = Math.min(this.gridSize - 1, nextY + 1); break;
                    case 'left': nextX = Math.max(0, nextX - 1); break;
                }

                const isObstacle = this.obstacles.some(obs => obs.x === nextX && obs.y === nextY);
                if (isObstacle) {
                    return [x, y];
                }
                return [nextX, nextY];
            }

            getReward(x, y) {
                if (x === this.goalPos.x && y === this.goalPos.y) return 100;
                const isObstacle = this.obstacles.some(obs => obs.x === x && obs.y === y);
                if (isObstacle) return -100;
                return -1;
            }

            updateAgentPosition(x, y) {
                this.grid[this.agentPos.y][this.agentPos.x].classList.remove('agent');
                this.grid[this.agentPos.y][this.agentPos.x].removeAttribute('aria-label');

                const currentCell = this.grid[this.agentPos.y][this.agentPos.x];
                if (this.agentPos.x === this.goalPos.x && this.agentPos.y === this.goalPos.y) {
                    currentCell.textContent = 'G';
                    currentCell.setAttribute('aria-label', 'Goal');
                } else if (this.obstacles.some(obs => obs.x === this.agentPos.x && obs.y === this.agentPos.y)) {
                    currentCell.textContent = 'X';
                    currentCell.setAttribute('aria-label', 'Obstacle');
                } else if (this.policyShown) {
                    const state = `${this.agentPos.x},${this.agentPos.y}`;
                    const qValues = this.qTable[state];
                    const bestAction = Object.keys(qValues).reduce((a, b) => qValues[a] > qValues[b] ? a : b);
                    let arrow = '';
                    switch (bestAction) {
                        case 'up': arrow = '‚Üë'; break;
                        case 'right': arrow = '‚Üí'; break;
                        case 'down': arrow = '‚Üì'; break;
                        case 'left': arrow = '‚Üê'; break;
                    }
                    currentCell.textContent = arrow;
                    currentCell.className = 'grid-cell';
                    if (this.showQValues) {
                        const bestValue = qValues[bestAction].toFixed(1);
                        currentCell.setAttribute('data-value', bestValue);
                    } else {
                        currentCell.removeAttribute('data-value');
                    }
                } else if (this.heatmapMode) {
                    this.showValueHeatmap();
                } else {
                    currentCell.textContent = '';
                }

                this.agentPos.x = x;
                this.agentPos.y = y;
                this.grid[y][x].classList.add('agent');
                this.grid[y][x].setAttribute('aria-label', 'Agent');
            }

            resetEnvironment() {
                clearInterval(this.learningInterval); // Clear any old interval if it exists
                this.isLearning = false; // Crucial to stop async loops
                document.getElementById('startLearningBtn').textContent = 'Start Learning';
                document.getElementById('startLearningBtn').disabled = false;
                document.getElementById('resetBtn').disabled = false;
                document.getElementById('stepByStepBtn').disabled = false;
                document.getElementById('statusText').textContent = 'Environment reset. Agent is ready to learn.';
                document.getElementById('learningProgress').style.width = '0%';

                // Reset stepMode button text based on current stepMode state
                const stepBtn = document.getElementById('stepByStepBtn');
                if (this.stepMode) {
                    stepBtn.textContent = 'Step-by-Step Mode (Active)';
                } else {
                    stepBtn.textContent = 'Step-by-Step';
                }

                this.initGrid();
                this.currentEpisode = 0;
                this.episodeHistory = [];
                this.episodeStats = []; // Also reset episode stats array
                this.resetStats(); // Recalculate and update stats display
                this.currentStepInEpisodeDisplay = 0; // Reset for step mode
                this.updateAgentPosition(0, 0);
            }

            showPolicy() {
                if (this.isLearning) return;
                for (let y = 0; y < this.gridSize; y++) {
                    for (let x = 0; x < this.gridSize; x++) {
                        const cell = this.grid[y][x];
                        if (cell.classList.contains('obstacle') || cell.classList.contains('goal')) continue;
                        cell.classList.remove('agent');
                        const state = `${x},${y}`;
                        const qValues = this.qTable[state];
                        const bestAction = Object.keys(qValues).reduce((a, b) => qValues[a] > qValues[b] ? a : b);
                        let arrow = '';
                        switch (bestAction) {
                            case 'up': arrow = '‚Üë'; break;
                            case 'right': arrow = '‚Üí'; break;
                            case 'down': arrow = '‚Üì'; break;
                            case 'left': arrow = '‚Üê'; break;
                        }
                        cell.textContent = arrow;
                        cell.classList.add('policy-arrow');
                        if (this.showQValues) {
                            const bestValue = qValues[bestAction].toFixed(1);
                            cell.setAttribute('data-value', bestValue);
                        } else {
                            cell.removeAttribute('data-value');
                        }
                    }
                }
                this.updateAgentPosition(0, 0);
                document.getElementById('statusText').textContent = 'Showing learned policy. Arrows indicate the best action in each state.';
                this.policyShown = true;
            }

            toggleStepMode() {
                this.stepMode = !this.stepMode;
                const stepBtn = document.getElementById('stepByStepBtn');

                if (this.isLearning) {
                    if (this.stepMode) {
                        // Switched TO step mode while continuous learning was active.
                        // The continuous learning (runAsyncEpisodeLoop) will see this.isLearning as false (set below) and stop.
                        // Then we prepare for step-by-step display.
                        this.isLearning = false; // Stop continuous loop
                        document.getElementById('startLearningBtn').disabled = false;
                        document.getElementById('resetBtn').disabled = false;

                        // Generate history up to current point if needed, or just restart for simplicity.
                        // For now, assume we'll start step-mode fresh or from where Q-table is.
                        // To show existing Q-table in step mode, pre-generate history:
                        this.episodeHistory = [];
                        for (let i = 0; i < this.maxEpisodes; i++) {
                            const episodeData = this.generateEpisodeData();
                            this.episodeHistory.push(episodeData);
                        }
                        this.currentEpisode = 0;
                        this.currentStepInEpisodeDisplay = 0;
                        this.isLearning = true; // Set for displayEpisodeStepByStep
                        this.displayEpisodeStepByStep();
                        // Buttons are managed by displayEpisodeStepByStep

                    } else {
                        // Switched FROM step mode (e.g. was "Next Action", now wants continuous)
                        // Stop the current step-by-step display.
                        this.isLearning = false; // This will stop displayEpisodeStepByStep
                        stepBtn.textContent = 'Step-by-Step';
                        stepBtn.disabled = false;
                        document.getElementById('startLearningBtn').textContent = 'Start Learning';
                        document.getElementById('startLearningBtn').disabled = false;
                        document.getElementById('resetBtn').disabled = false;
                        document.getElementById('statusText').textContent = 'Step-by-step mode exited. Click Start Learning for continuous mode.';
                    }
                } else {
                    // Not currently learning, just toggling the mode for next "Start Learning"
                    if (this.stepMode) {
                        stepBtn.textContent = 'Step-by-Step Mode (Active)';
                    } else {
                        stepBtn.textContent = 'Step-by-Step';
                    }
                    stepBtn.disabled = false;
                }
            }

            // New function for step-by-step mode's UI and progression
            displayEpisodeStepByStep() {
                if (!this.isLearning || !this.stepMode || !this.episodeHistory || this.episodeHistory.length === 0) {
                    // If not learning, or not in step mode, or no history, ensure buttons are sensible
                    if (!this.isLearning) {
                        document.getElementById('startLearningBtn').disabled = false;
                        document.getElementById('resetBtn').disabled = false;
                        const stepBtn = document.getElementById('stepByStepBtn');
                        stepBtn.textContent = this.stepMode ? 'Step-by-Step Mode (Active)' : 'Step-by-Step';
                        stepBtn.disabled = false;
                    }
                    return;
                }

                const stepBtn = document.getElementById('stepByStepBtn');
                stepBtn.disabled = false; // Always enable the step button when this function is active
                document.getElementById('startLearningBtn').disabled = true; // Disable start while stepping
                document.getElementById('resetBtn').disabled = false; // Allow reset


                if (this.currentEpisode >= this.maxEpisodes || this.currentEpisode >= this.episodeHistory.length) {
                    this.isLearning = false;
                    document.getElementById('startLearningBtn').textContent = 'Start Learning';
                    document.getElementById('startLearningBtn').disabled = false;
                    document.getElementById('statusText').textContent = 'All episodes shown. Learning complete!';
                    stepBtn.textContent = 'Step-by-Step Mode (Active)'; // Or 'Step-by-Step' if want to exit mode
                    this.updateStatsDisplay(); // Final stats update
                    return;
                }

                const episodeData = this.episodeHistory[this.currentEpisode];

                if (this.currentStepInEpisodeDisplay < episodeData.steps.length) {
                    const step = episodeData.steps[this.currentStepInEpisodeDisplay];
                    this.updateAgentPosition(step.x, step.y); // This updates this.agentPos
                    document.getElementById('statusText').textContent =
                        `Episode ${this.currentEpisode + 1}/${this.maxEpisodes}, Step ${this.currentStepInEpisodeDisplay + 1}/${episodeData.steps.length}. Action: ${step.action}, Reward: ${step.reward}`;
                    stepBtn.textContent = 'Next Action';
                    // Clicking "Next Action" will call toggleStepMode, which if isLearning & stepMode, re-calls this function after ++currentStepInEpisodeDisplay.
                    // So, we just set up the state for the next click.
                } else {
                    // End of current episode
                    this.updateEpisodeStats(episodeData, this.currentEpisode);
                    const progress = ((this.currentEpisode + 1) / this.maxEpisodes) * 100;
                    document.getElementById('learningProgress').style.width = `${progress}%`;
                    this.updateStatsDisplay();

                    this.currentEpisode++;
                    this.currentStepInEpisodeDisplay = 0;

                    if (this.currentEpisode < this.maxEpisodes && this.currentEpisode < this.episodeHistory.length) {
                        document.getElementById('statusText').textContent = `Episode ${this.currentEpisode} actions shown. Click "Next Episode" to view Episode ${this.currentEpisode + 1}.`;
                        stepBtn.textContent = 'Next Episode';
                    } else {
                        this.isLearning = false;
                        document.getElementById('startLearningBtn').textContent = 'Start Learning';
                        document.getElementById('startLearningBtn').disabled = false;
                        document.getElementById('statusText').textContent = 'All episodes shown. Learning complete!';
                        stepBtn.textContent = 'Step-by-Step Mode (Active)'; // Or 'Step-by-Step'
                    }
                }
            }

            toggleParamControls() {
                this.paramControlsDisplayed = !this.paramControlsDisplayed;
                if (this.paramControlsDisplayed) {
                    document.getElementById('paramControls').classList.remove('hidden');
                } else {
                    document.getElementById('paramControls').classList.add('hidden');
                }
                document.getElementById('adjustParamsBtn').textContent = this.paramControlsDisplayed ? 'Hide Parameters' : 'Adjust Parameters';
            }
        }

        // GridWorld Agent class to handle RL algorithms
        class GridWorldAgent {
            constructor(gridSize = 5) {
                this.gridSize = gridSize;
                this.learningRate = 0.1;
                this.discountFactor = 0.9;
                this.explorationRate = 0.3;
                this.episodes = 100;
                this.currentEpisode = 0;
                this.position = { x: 0, y: 0 };
                this.goalPosition = { x: gridSize - 1, y: gridSize - 1 };
                this.obstacles = [];
                this.qTable = this.initQTable();
                this.isLearning = false;
                this.stepByStep = false;

                // Make the agent globally accessible
                window.gridWorldAgent = this;
            }

            // Initialize Q-table with zeros
            initQTable() {
                const qTable = {};
                for (let y = 0; y < this.gridSize; y++) {
                    for (let x = 0; x < this.gridSize; x++) {
                        const stateKey = `${x},${y}`;
                        qTable[stateKey] = {
                            up: 0,
                            right: 0,
                            down: 0,
                            left: 0
                        };
                    }
                }
                return qTable;
            }

            // Update the agent's parameters from UI controls
            updateParameters(params) {
                if (params.learningRate !== undefined) this.learningRate = params.learningRate;
                if (params.discountFactor !== undefined) this.discountFactor = params.discountFactor;
                if (params.explorationRate !== undefined) this.explorationRate = params.explorationRate;
                if (params.episodes !== undefined) this.episodes = params.episodes;

                this.updateUI();
                return this;
            }

            // Update UI with current parameters
            updateUI() {
                const statusText = document.getElementById('statusText');
                if (statusText && !this.isLearning) {
                    statusText.textContent = `Agent configured: Œ±=${this.learningRate}, Œ≥=${this.discountFactor}, Œµ=${this.explorationRate}, episodes=${this.episodes}`;
                }

                // Update progress bar
                if (this.isLearning) {
                    const progressBar = document.getElementById('learningProgress');
                    if (progressBar) {
                        const progress = (this.currentEpisode / this.episodes) * 100;
                        progressBar.style.width = `${progress}%`;
                    }
                }
            }
        }

        // Initialize the agent when page loads
        document.addEventListener('DOMContentLoaded', function () {
            // Create the agent
            const agent = new GridWorldAgent(5);

            // Initialize UI with agent parameter values
            const learningRateSlider = document.getElementById('learningRate');
            const discountFactorSlider = document.getElementById('discountFactor');
            const explorationRateSlider = document.getElementById('explorationRate');
            const episodesSlider = document.getElementById('episodes');

            if (learningRateSlider) learningRateSlider.value = agent.learningRate;
            if (discountFactorSlider) discountFactorSlider.value = agent.discountFactor;
            if (explorationRateSlider) explorationRateSlider.value = agent.explorationRate;
            if (episodesSlider) episodesSlider.value = agent.episodes;

            // Initialize grid world UI
            initializeGridWorld();
        });

        // Function to initialize grid world UI
        function initializeGridWorld() {
            const gridWorld = document.getElementById('gridWorld');
            if (!gridWorld) return;

            // Clear existing grid
            gridWorld.innerHTML = '';

            // Create grid cells
            const agent = window.gridWorldAgent;
            if (!agent) return;

            for (let y = 0; y < agent.gridSize; y++) {
                for (let x = 0; x < agent.gridSize; x++) {
                    const cell = document.createElement('div');
                    cell.className = 'grid-cell';
                    cell.dataset.x = x;
                    cell.dataset.y = y;

                    // Set goal cell
                    if (x === agent.goalPosition.x && y === agent.goalPosition.y) {
                        cell.classList.add('goal');
                        cell.textContent = 'G';
                    }

                    // Set agent position
                    if (x === agent.position.x && y === agent.position.y) {
                        cell.classList.add('agent');
                    }

                    gridWorld.appendChild(cell);
                }
            }

            // Update status
            agent.updateUI();
        }

        // Improved search functionality
        const searchInput = document.getElementById('searchInput');
        let currentHighlightIndex = -1;
        let highlights = [];

        if (searchInput) {
            searchInput.addEventListener('input', function () {
                const query = this.value.toLowerCase();
                currentHighlightIndex = -1;

                if (query.length < 2) {
                    document.querySelectorAll('.search-highlight').forEach(el => {
                        if (el.parentNode) el.outerHTML = el.innerHTML;
                    });
                    document.querySelectorAll('.sidebar-nav a').forEach(link => {
                        link.style.backgroundColor = '';
                    });
                    const searchNavControls = document.getElementById('searchNavControls');
                    if (searchNavControls) searchNavControls.style.display = 'none';
                    const searchResultMsg = document.getElementById('searchResultMsg');
                    if (searchResultMsg) searchResultMsg.style.display = 'none';
                    return;
                }

                const sections = document.querySelectorAll('section.card'); // More specific selector for content sections
                let hasResults = false;

                document.querySelectorAll('.search-highlight').forEach(el => {
                    if (el.parentNode) el.outerHTML = el.innerHTML;
                });
                document.querySelectorAll('.sidebar-nav a').forEach(link => {
                    link.style.backgroundColor = '';
                });

                sections.forEach(section => {
                    const sectionText = section.textContent.toLowerCase();
                    const sectionId = section.id;
                    if (sectionText.includes(query)) {
                        const sidebarLink = document.querySelector(`.sidebar-nav a[href="#${sectionId}"]`);
                        if (sidebarLink) sidebarLink.style.backgroundColor = 'rgba(59, 130, 246, 0.1)';
                        hasResults = true;
                        highlightText(section, query);
                    }
                });

                highlights = Array.from(document.querySelectorAll('.search-highlight'));
                let searchResultMsg = document.getElementById('searchResultMsg');
                if (!searchResultMsg) {
                    searchResultMsg = document.createElement('div');
                    searchResultMsg.id = 'searchResultMsg';
                    searchResultMsg.className = 'info-box';
                    searchResultMsg.style.marginTop = '0.5rem';
                    searchResultMsg.style.marginBottom = '0.5rem';
                    searchResultMsg.style.fontSize = '0.9rem';
                    searchResultMsg.setAttribute('aria-live', 'polite');
                    const searchContainer = document.querySelector('.search-container');
                    if (searchContainer && searchContainer.parentNode) { // Ensure searchContainer exists and is in DOM
                        searchContainer.parentNode.insertBefore(searchResultMsg, searchContainer.nextSibling);
                    }
                }

                if (highlights.length > 0) { // Changed from hasResults to highlights.length
                    searchResultMsg.innerHTML = `Found <strong>${highlights.length}</strong> matches for "${query}"`;
                    searchResultMsg.style.display = 'block';
                    let searchNavControls = document.getElementById('searchNavControls');
                    if (!searchNavControls) {
                        searchNavControls = document.createElement('div');
                        searchNavControls.id = 'searchNavControls';
                        searchNavControls.style.display = 'flex';
                        searchNavControls.style.gap = '0.5rem';
                        searchNavControls.style.marginTop = '0.5rem';

                        const prevButton = document.createElement('button');
                        prevButton.id = 'prevResult';
                        prevButton.className = 'btn';
                        prevButton.style.padding = '0.2rem 0.5rem';
                        prevButton.style.fontSize = '0.8rem';
                        prevButton.innerHTML = '&larr; Previous';
                        prevButton.addEventListener('click', navigateToPrevResult);

                        const nextButton = document.createElement('button');
                        nextButton.id = 'nextResult';
                        nextButton.className = 'btn';
                        nextButton.style.padding = '0.2rem 0.5rem';
                        nextButton.style.fontSize = '0.8rem';
                        nextButton.innerHTML = 'Next &rarr;';
                        nextButton.addEventListener('click', navigateToNextResult);

                        const resultCounter = document.createElement('span');
                        resultCounter.id = 'resultCounter';
                        resultCounter.style.margin = 'auto 0';
                        resultCounter.style.fontSize = '0.8rem';

                        searchNavControls.appendChild(prevButton);
                        searchNavControls.appendChild(resultCounter);
                        searchNavControls.appendChild(nextButton);
                        searchResultMsg.appendChild(searchNavControls); // Append controls to message div
                    }
                    searchNavControls.style.display = 'flex';
                    updateResultCounter();
                } else {
                    searchResultMsg.innerHTML = `No results found for "${query}"`;
                    searchResultMsg.style.display = 'block';
                    const searchNavControls = document.getElementById('searchNavControls');
                    if (searchNavControls) searchNavControls.style.display = 'none';
                }
            });

            searchInput.addEventListener('keydown', function (e) {
                if (e.key === 'Enter') {
                    e.preventDefault();
                    if (highlights.length > 0) { // Only navigate if there are results
                        if (e.shiftKey) navigateToPrevResult();
                        else navigateToNextResult();
                    }
                } else if (e.key === 'Escape') {
                    this.value = '';
                    this.dispatchEvent(new Event('input'));
                    this.blur();
                }
            });

            searchInput.addEventListener('change', function () { // Changed from 'change' to 'input' for better UX, but keeping 'change' as per original if specific. Assuming 'input' was intended.
                if (this.value.trim().length >= 2 && highlights.length > 0 && currentHighlightIndex === -1) {
                    setTimeout(() => navigateToNextResult(), 100);
                }
            });

            const searchContainer = document.querySelector('.search-container');
            if (searchContainer) {
                const clearSearchBtn = document.createElement('button');
                clearSearchBtn.id = 'clearSearchBtn';
                clearSearchBtn.className = 'search-clear'; // Assuming this class is styled elsewhere
                clearSearchBtn.innerHTML = '&times;';
                clearSearchBtn.setAttribute('aria-label', 'Clear search');
                clearSearchBtn.style.position = 'absolute';
                clearSearchBtn.style.right = '0.75rem';
                clearSearchBtn.style.top = '50%';
                clearSearchBtn.style.transform = 'translateY(-50%)';
                clearSearchBtn.style.background = 'transparent';
                clearSearchBtn.style.border = 'none';
                clearSearchBtn.style.color = 'var(--text-light)'; // Ensure CSS var is defined
                clearSearchBtn.style.fontSize = '1.2rem';
                clearSearchBtn.style.cursor = 'pointer';
                clearSearchBtn.style.display = 'none';
                searchContainer.style.position = 'relative'; // Needed for absolute positioning of clear button
                searchContainer.appendChild(clearSearchBtn);

                clearSearchBtn.addEventListener('click', () => {
                    searchInput.value = '';
                    searchInput.dispatchEvent(new Event('input'));
                    searchInput.focus();
                });
                searchInput.addEventListener('input', function () {
                    clearSearchBtn.style.display = this.value ? 'block' : 'none';
                });
            }
        }

        function navigateToNextResult() {
            if (highlights.length === 0) return;
            if (highlights[currentHighlightIndex]) highlights[currentHighlightIndex].classList.remove('search-highlight-current');
            currentHighlightIndex = (currentHighlightIndex + 1) % highlights.length;
            navigateToResult(currentHighlightIndex);
        }

        function navigateToPrevResult() {
            if (highlights.length === 0) return;
            if (highlights[currentHighlightIndex]) highlights[currentHighlightIndex].classList.remove('search-highlight-current');
            currentHighlightIndex = (currentHighlightIndex - 1 + highlights.length) % highlights.length;
            navigateToResult(currentHighlightIndex);
        }

        function navigateToResult(index) {
            if (index < 0 || index >= highlights.length) return;
            const currentElement = highlights[index];
            currentElement.classList.add('search-highlight-current');
            currentElement.scrollIntoView({ behavior: 'smooth', block: 'center' });
            // currentElement.focus(); // Focusing can sometimes be disruptive, make optional or test thoroughly
            updateResultCounter();
        }

        function updateResultCounter() {
            const counter = document.getElementById('resultCounter');
            if (!counter) return;
            if (highlights.length > 0 && currentHighlightIndex !== -1) { // Ensure currentHighlightIndex is valid
                counter.textContent = `${currentHighlightIndex + 1} of ${highlights.length}`;
            } else if (highlights.length > 0 && currentHighlightIndex === -1) {
                counter.textContent = `0 of ${highlights.length}`; // Show 0 of X before first navigation
            }
            else {
                counter.textContent = '0 of 0';
            }
        }

        function highlightText(element, query) {
            if (element.nodeType === Node.TEXT_NODE) {
                const match = element.nodeValue.toLowerCase().indexOf(query);
                if (match !== -1) {
                    const span = document.createElement('span');
                    span.className = 'search-highlight';
                    // Styles for .search-highlight should be in CSS for maintainability
                    // span.style.backgroundColor = 'rgba(59, 130, 246, 0.3)'; 
                    // span.style.borderRadius = '2px';
                    // span.style.padding = '0 2px';
                    span.setAttribute('tabindex', '-1'); // Make focusable if needed, but generally not individual spans

                    const before = element.nodeValue.substring(0, match);
                    const middle = element.nodeValue.substring(match, match + query.length);
                    const after = element.nodeValue.substring(match + query.length);

                    if (element.parentNode) {
                        if (before) element.parentNode.insertBefore(document.createTextNode(before), element);
                        span.textContent = middle;
                        element.parentNode.insertBefore(span, element);
                        if (after) element.parentNode.insertBefore(document.createTextNode(after), element);
                        element.parentNode.removeChild(element);
                        // Recursively highlight in the "after" part if necessary
                        if (after.toLowerCase().includes(query) && span.nextSibling && span.nextSibling.nodeType === Node.TEXT_NODE) {
                            highlightText(span.nextSibling, query);
                        }
                    }
                    return; // Processed this text node
                }
            } else if (element.nodeType === Node.ELEMENT_NODE) {
                if (element.matches('script, style, textarea, input, button, code, .search-highlight')) {
                    return; // Skip these elements
                }
                // Iterate over a copy of childNodes because the collection can change
                Array.from(element.childNodes).forEach(child => highlightText(child, query));
            }
        }

        // Add style for current highlight (better to put in main CSS)
        const searchHighlightStyle = document.createElement('style');
        searchHighlightStyle.textContent = `
    .search-highlight {
        background-color: rgba(59, 130, 246, 0.3);
        border-radius: 2px;
        padding: 0 2px;
    }
    .search-highlight-current {
        background-color: rgba(59, 130, 246, 0.6) !important;
        box-shadow: 0 0 0 2px rgba(59, 130, 246, 0.8);
        /* transform: scale(1.05); */ /* scale can cause layout shifts */
        transition: background-color 0.2s ease, box-shadow 0.2s ease;
    }
`;
        document.head.appendChild(searchHighlightStyle);

        document.addEventListener('keydown', function (e) {
            const focusedElement = document.activeElement;
            if (focusedElement && focusedElement.classList && focusedElement.classList.contains('search-highlight')) {
                if (e.key === 'Enter') {
                    e.preventDefault();
                    if (highlights.length > 0) {
                        if (e.shiftKey) navigateToPrevResult();
                        else navigateToNextResult();
                    }
                }
            }
        });


        // Chat assistant functionality
        const chatFab = document.getElementById('chatFab');
        const chatContainer = document.getElementById('chatContainer');
        const chatClose = document.getElementById('chatClose');
        const chatMessages = document.getElementById('chatMessages');
        const chatInput = document.getElementById('chatInput');
        const chatSend = document.getElementById('chatSend');

        if (chatFab) {
            chatFab.addEventListener('click', () => {
                if (chatContainer) chatContainer.classList.toggle('active');
                if (chatContainer && chatContainer.classList.contains('active') && chatInput) chatInput.focus();
            });
        }
        if (chatClose) {
            chatClose.addEventListener('click', () => {
                if (chatContainer) chatContainer.classList.remove('active');
            });
        }
        if (chatSend) chatSend.addEventListener('click', sendChatMessage);
        if (chatInput) {
            chatInput.addEventListener('keypress', (e) => {
                if (e.key === 'Enter' && !e.shiftKey) {
                    e.preventDefault();
                    sendChatMessage();
                }
            });
            chatInput.addEventListener('input', function () {
                this.style.height = 'auto';
                this.style.height = (this.scrollHeight) + 'px';
            });
        }

        function enhancedChatResponses(message) {
            const msgLower = message.toLowerCase();
            // (Original enhancedChatResponses logic from line 1575 to 1613 in the provided file)
            if (msgLower.includes('q-learning')) {
                return "Q-learning is a value-based reinforcement learning algorithm that learns the value of actions in states without requiring a model of the environment. It\'s an off-policy algorithm that uses the Bellman equation to update its Q-values. The update rule is: Q(s,a) ‚Üê Q(s,a) + Œ±[r + Œ≥¬∑max Q(s\',a\') - Q(s,a)]. Would you like to know more about its implementation or applications?";
            } else if (msgLower.includes('policy gradient')) {
                return "Policy gradient methods directly optimize the policy by following the gradient of expected reward. They work well with continuous action spaces and can learn stochastic policies. Popular algorithms include REINFORCE, PPO, and TRPO. The core idea is to increase the probability of actions that lead to higher rewards. Would you like to see an example?";
            } else if (msgLower.includes('exploration') || msgLower.includes('exploit')) {
                return "The exploration-exploitation dilemma is fundamental to RL. Exploration discovers new strategies while exploitation uses known good actions. Common approaches include: Œµ-greedy (random actions with probability Œµ), softmax (probability proportional to estimated value), UCB (optimism in the face of uncertainty), and Thompson sampling (Bayesian approach). The demo in this page uses Œµ-greedy - try adjusting the exploration rate parameter!";
            } else if (msgLower.includes('example') || msgLower.includes('application')) {
                return "RL applications include: 1) Game playing: AlphaGo, AlphaZero, and agents that master Atari games. 2) Robotics: teaching robots to walk, manipulate objects, and navigate environments. 3) Resource management: traffic light control, HVAC systems, and datacenter cooling. 4) Finance: portfolio management and algorithmic trading. 5) Healthcare: personalized treatment recommendations and drug discovery. Which area interests you most?";
            } else if (msgLower.includes('actor-critic')) {
                return "Actor-Critic methods combine policy-based and value-based approaches. The 'actor' learns a policy to select actions, while the 'critic' evaluates those actions. This reduces variance in policy updates while maintaining the ability to learn in continuous action spaces. Popular variants include A2C/A3C, DDPG, TD3, and PPO. The 'advantage' function (A = Q - V) is often used to determine how much better an action is compared to the average.";
            } else if (msgLower.includes('deep') || msgLower.includes('dqn')) {
                return "Deep Reinforcement Learning uses neural networks to handle complex state spaces. DQN revolutionized the field by successfully playing Atari games from raw pixels. Key innovations include: 1) Experience replay to break correlations in training data, 2) Target networks to stabilize learning, 3) Double DQN to reduce overestimation bias, and 4) Dueling architecture to separate state value and action advantage. Check out the 'Popular Algorithms' section for more!";
            } else if (msgLower.includes('reward') || msgLower.includes('function')) {
                return "The reward function defines the goal in RL by specifying which outcomes are desirable. Good reward design is crucial and challenging. Sparse rewards (e.g., only at goal states) make learning difficult, while poorly designed rewards can lead to 'reward hacking' where agents find unexpected ways to maximize reward without achieving the intended goal. Reward shaping adds intermediate rewards to guide learning. What specific aspect of reward functions are you curious about?";
            } else if (msgLower.includes('markov') || msgLower.includes('mdp')) {
                return "Markov Decision Processes (MDPs) provide the mathematical framework for RL, consisting of states, actions, transition probabilities, rewards, and a discount factor. The Markov property means the future depends only on the current state, not the history. If the state isn\'t fully observable, we use Partially Observable MDPs (POMDPs). The Grid World demo on this page is a simple MDP - each cell is a state, and the agent can take four actions (up, down, left, right).";
            } else if (msgLower.includes('grid') || msgLower.includes('demo')) {
                return "The Grid World demo illustrates Q-learning in action. The agent (ü§ñ) learns to navigate to the goal (G) while avoiding obstacles (X). Try clicking 'Start Learning' to watch the process, then 'Show Learned Policy' to see the optimal path. You can adjust parameters like learning rate (how quickly new information is incorporated), discount factor (importance of future rewards), and exploration rate (balancing exploration and exploitation). Click 'Adjust Parameters' to experiment!";
            } else if (msgLower.includes('hello') || msgLower.includes('hi') || msgLower.includes('hey')) {
                return "Hello! I\'m your RL assistant. I can help you understand reinforcement learning concepts, algorithms, and applications. Try asking about Q-learning, policy gradients, the exploration-exploitation dilemma, or check out the interactive Grid World demo on this page. What would you like to know about?";
            } else if (msgLower.includes('thank')) {
                return "You\'re welcome! Feel free to ask if you have any other questions about reinforcement learning. Don\'t forget to try the interactive demo and explore different sections of this page for more information.";
            } else {
                return "Thanks for your question about reinforcement learning! I can help with topics like algorithms (Q-learning, DQN, policy gradients), concepts (exploration-exploitation, MDPs, reward functions), or applications (games, robotics, finance). Try interacting with the Grid World demo to see RL in action! Could you provide more details about what you\'d like to know?";
            }
        }

        function sendChatMessage() {
            if (!chatInput || !chatMessages) return;
            const message = chatInput.value.trim();
            if (!message) return;

            addChatMessage(message, 'user');
            chatInput.value = '';
            chatInput.style.height = 'auto'; // Reset height

            const typingIndicator = document.createElement('div');
            typingIndicator.className = 'chat-message bot';
            typingIndicator.id = 'typing-indicator';
            typingIndicator.innerHTML = '<span class="typing-dot"></span><span class="typing-dot"></span><span class="typing-dot"></span>';
            chatMessages.appendChild(typingIndicator);
            chatMessages.scrollTop = chatMessages.scrollHeight;

            setTimeout(() => {
                const typingIndicatorEl = document.getElementById('typing-indicator');
                if (typingIndicatorEl) typingIndicatorEl.remove();
                const response = enhancedChatResponses(message);
                typeMessage(response, 'bot');
            }, 1000 + Math.random() * 1000);
        }

        function typeMessage(message, sender, speed = 30) {
            if (!chatMessages) return;
            const messageElement = document.createElement('div');
            messageElement.className = `chat-message ${sender}`;
            chatMessages.appendChild(messageElement);

            let i = 0;
            const typingInterval = setInterval(() => { // Renamed from typing to typingInterval
                if (i < message.length) {
                    messageElement.textContent += message.charAt(i);
                    i++;
                    chatMessages.scrollTop = chatMessages.scrollHeight;
                } else {
                    clearInterval(typingInterval);
                }
            }, speed);
        }

        function addChatMessage(message, sender) {
            if (!chatMessages) return;
            const messageElement = document.createElement('div');
            messageElement.className = `chat-message ${sender}`;
            messageElement.textContent = message;
            chatMessages.appendChild(messageElement);
            chatMessages.scrollTop = chatMessages.scrollHeight;
        }

        // Interactive guided tour
        class GuidedTour {
            constructor() {
                this.currentStep = 0;
                this.tourSteps = [
                    // (Original tourSteps array from line 1754 to 1818)
                    {
                        title: "Welcome to Reinforcement Learning!",
                        content: "This interactive tour will guide you through the key features of this page. Click 'Next' to continue or 'Close' to exit at any time.",
                        target: 'header', // Assuming <header> element exists
                        position: 'bottom-center'
                    },
                    {
                        title: "Dark Mode Toggle",
                        content: "Click this button to switch between light and dark mode, depending on your preference.",
                        target: 'themeToggle',
                        position: 'bottom-left'
                    },
                    {
                        title: "Navigation Sidebar",
                        content: "Use this sidebar to quickly navigate to different sections of the page.",
                        target: 'main-navigation', // ID set in enhanceAccessibility
                        position: 'right-center'
                    },
                    {
                        title: "Search Functionality",
                        content: "Search for any reinforcement learning concept or term. Try typing 'Q-learning' or 'reward' to find relevant content.",
                        target: 'searchInput',
                        position: 'bottom-center'
                    },
                    {
                        title: "Interactive Tabs",
                        content: "Many sections contain tabs to organize related content. Click on different tabs to explore various aspects of the topic.",
                        target: '.tabs-container', // Generic selector, ensure one exists or use specific ID like 'tab-components' if available
                        position: 'bottom-center'
                    },
                    {
                        title: "Grid World Demo",
                        content: "This interactive demo lets you see reinforcement learning in action. Click 'Start Learning' to watch the agent learn to navigate to the goal.",
                        target: '#gridWorld', // Target the grid itself or its container like '.interactive-demo' or an ID 'demo'
                        position: 'top-center'
                    },
                    {
                        title: "Demo Controls",
                        content: "Use these buttons to control the demo. You can start/stop learning, reset the environment, and visualize the learned policy.",
                        target: '.demo-controls', // Target the controls container or a specific button like 'startLearningBtn'
                        position: 'top-left'
                    },
                    {
                        title: "Chat Assistant",
                        content: "Have questions about reinforcement learning? Click this chat button to get help from the RL assistant.",
                        target: 'chatFab',
                        position: 'top-right'
                    },
                    {
                        title: "Settings Panel",
                        content: "Customize your experience with settings for appearance, simulation parameters, and accessibility.",
                        target: 'settingsBtn',
                        position: 'bottom-right'
                    },
                    {
                        title: "Keyboard Shortcuts",
                        content: "Power users can utilize keyboard shortcuts for faster navigation. Press '/' for search, 's' for settings, 'c' for chat, and 'd' for dark mode toggle.",
                        target: '.sidebar .info-box:last-of-type', // More specific for the shortcuts box if it's the last info-box in sidebar
                        position: 'right-center'
                    },
                    {
                        title: "Tour Complete!",
                        content: "You've completed the tour. Feel free to explore the page and learn about reinforcement learning. You can restart this tour anytime by clicking the info button in the bottom right corner.",
                        target: 'tutorialFab',
                        position: 'top-right'
                    }
                ];

                this.overlay = document.getElementById('tourOverlay');
                this.tooltip = document.getElementById('tourTooltip');
                this.titleEl = document.getElementById('tourTitle');
                this.contentEl = document.getElementById('tourContent');
                this.prevBtn = document.getElementById('tourPrev');
                this.nextBtn = document.getElementById('tourNext');
                this.closeBtn = document.getElementById('tourClose');

                // Ensure tour HTML elements exist before init
                if (this.overlay && this.tooltip && this.titleEl && this.contentEl && this.prevBtn && this.nextBtn && this.closeBtn) {
                    this.init();
                } else {
                    console.warn("Guided Tour HTML elements not found. Tour will not initialize.");
                }
            }

            init() {
                this.prevBtn.addEventListener('click', () => this.prevStep());
                this.nextBtn.addEventListener('click', () => this.nextStep());
                this.closeBtn.addEventListener('click', () => this.endTour());

                const tutorialFab = document.getElementById('tutorialFab');
                if (tutorialFab) {
                    tutorialFab.addEventListener('click', () => this.startTour());
                }

                document.addEventListener('keydown', (e) => {
                    if (e.key === 'Escape' && this.overlay.style.display === 'block') {
                        this.endTour();
                    }
                });
            }

            startTour() {
                this.currentStep = 0;
                this.overlay.style.display = 'block';
                this.showStep(this.currentStep);
            }

            endTour() {
                this.overlay.style.display = 'none';
                document.querySelectorAll('.tour-highlight').forEach(el => el.classList.remove('tour-highlight'));
            }

            prevStep() {
                if (this.currentStep > 0) {
                    this.currentStep--;
                    this.showStep(this.currentStep);
                }
            }

            nextStep() {
                if (this.currentStep < this.tourSteps.length - 1) {
                    this.currentStep++;
                    this.showStep(this.currentStep);
                } else {
                    this.endTour();
                }
            }

            showStep(stepIndex) {
                const step = this.tourSteps[stepIndex];
                this.titleEl.textContent = step.title;
                this.contentEl.textContent = step.content;
                this.prevBtn.style.visibility = (stepIndex === 0) ? 'hidden' : 'visible';
                this.nextBtn.textContent = (stepIndex === this.tourSteps.length - 1) ? 'Finish' : 'Next';

                let targetEl = typeof step.target === 'string' ? (document.getElementById(step.target) || document.querySelector(step.target)) : step.target;

                this.positionTooltip(targetEl, step.position); // Position tooltip first

                if (targetEl && typeof targetEl.scrollIntoView === 'function') {
                    try {
                        targetEl.scrollIntoView({ behavior: 'smooth', block: 'center', inline: 'center' });
                    } catch (e) { // Fallback for elements that can't be scrolled (e.g. SVG or not in flow)
                        window.scrollTo({ top: targetEl.getBoundingClientRect().top + window.scrollY - (window.innerHeight / 2), behavior: 'smooth' });
                    }
                }
                this.highlightTarget(targetEl);
            }

            positionTooltip(targetEl, position) {
                const tooltipRect = this.tooltip.getBoundingClientRect();
                let top, left;

                if (!targetEl) { // Default position if target is not found
                    top = (window.innerHeight - tooltipRect.height) / 2;
                    left = (window.innerWidth - tooltipRect.width) / 2;
                } else {
                    const targetRect = targetEl.getBoundingClientRect();
                    switch (position) {
                        case 'top-left': top = targetRect.top - tooltipRect.height - 10; left = targetRect.left; break;
                        case 'top-center': top = targetRect.top - tooltipRect.height - 10; left = targetRect.left + (targetRect.width / 2) - (tooltipRect.width / 2); break;
                        case 'top-right': top = targetRect.top - tooltipRect.height - 10; left = targetRect.right - tooltipRect.width; break;
                        case 'right-top': top = targetRect.top; left = targetRect.right + 10; break;
                        case 'right-center': top = targetRect.top + (targetRect.height / 2) - (tooltipRect.height / 2); left = targetRect.right + 10; break;
                        case 'right-bottom': top = targetRect.bottom - tooltipRect.height; left = targetRect.right + 10; break;
                        case 'bottom-right': top = targetRect.bottom + 10; left = targetRect.right - tooltipRect.width; break;
                        case 'bottom-center': top = targetRect.bottom + 10; left = targetRect.left + (targetRect.width / 2) - (tooltipRect.width / 2); break;
                        case 'bottom-left': top = targetRect.bottom + 10; left = targetRect.left; break;
                        case 'left-bottom': top = targetRect.bottom - tooltipRect.height; left = targetRect.left - tooltipRect.width - 10; break;
                        case 'left-center': top = targetRect.top + (targetRect.height / 2) - (tooltipRect.height / 2); left = targetRect.left - tooltipRect.width - 10; break;
                        case 'left-top': top = targetRect.top; left = targetRect.left - tooltipRect.width - 10; break;
                        default: // bottom-center as default
                            top = targetRect.bottom + 10;
                            left = targetRect.left + (targetRect.width / 2) - (tooltipRect.width / 2);
                    }
                }

                // Keep tooltip within viewport
                top = Math.max(10, Math.min(top, window.innerHeight - tooltipRect.height - 10));
                left = Math.max(10, Math.min(left, window.innerWidth - tooltipRect.width - 10));

                this.tooltip.style.top = `${top + window.scrollY}px`; // Add scrollY for absolute positioning
                this.tooltip.style.left = `${left + window.scrollX}px`; // Add scrollX
            }

            highlightTarget(targetEl) {
                document.querySelectorAll('.tour-highlight').forEach(el => el.classList.remove('tour-highlight'));
                if (targetEl) {
                    targetEl.classList.add('tour-highlight');
                }
                // Ensure tour styles are present
                if (!document.getElementById('tour-styles')) {
                    const style = document.createElement('style');
                    style.id = 'tour-styles';
                    // Styles for .tour-highlight and overlay interaction
                    // These styles create a "cutout" effect for the highlighted element
                    style.textContent = `
                .tour-highlight {
                    position: relative; /* Needed for z-index and pseudo-elements if used for border */
                    z-index: 10001; /* Above the overlay */
                    box-shadow: 0 0 0 3px var(--primary), 0 0 10px rgba(0,0,0,0.3); /* Highlight border */
                    border-radius: 4px;
                    transition: box-shadow 0.3s ease-in-out;
                }
                /* Ensure overlay is below the tooltip but above other content */
                #tourOverlay { position: fixed; top: 0; left: 0; width: 100%; height: 100%; background: rgba(0,0,0,0.5); z-index: 10000; }
                #tourTooltip { position: absolute; z-index: 10002; /* Above highlight and overlay */ }
            `;
                    document.head.appendChild(style);
                }
            }
        }

        // Function to initialize and update settings panel UI elements
        function initializeSettingsUI() {
            const settings = {
                darkModeToggle: document.getElementById('darkModeToggle'),
                fontSizeSlider: document.getElementById('fontSize'),
                simulationSpeedSlider: document.getElementById('simulationSpeed'),
                showQValuesToggle: document.getElementById('showQValues'),
                highContrastToggle: document.getElementById('highContrast'),
                reducedMotionToggle: document.getElementById('reducedMotion'),
                showDefinitionsToggle: document.getElementById('showDefinitions'),
                showAdvancedToggle: document.getElementById('showAdvanced'),
                // For displaying values
                fontSizeValueDisplay: null, // Assuming one might be added later for font size
                simulationSpeedValueDisplay: document.getElementById('simulationSpeedValueDisplay')
            };

            // Initial state sync and event listeners for sliders that display their values
            if (settings.simulationSpeedSlider && settings.simulationSpeedValueDisplay) {
                settings.simulationSpeedValueDisplay.textContent = settings.simulationSpeedSlider.value;
                settings.simulationSpeedSlider.addEventListener('input', (event) => {
                    const newSpeed = event.target.value;
                    settings.simulationSpeedValueDisplay.textContent = newSpeed;
                    if (window.gridWorldDemo && typeof window.gridWorldDemo.setSimulationSpeed === 'function') {
                        window.gridWorldDemo.setSimulationSpeed(parseInt(newSpeed));
                    }
                });
            }

            if (settings.fontSizeSlider) { // Example if font size display was added
                // settings.fontSizeValueDisplay.textContent = settings.fontSizeSlider.value + 'px'; 
                // settings.fontSizeSlider.addEventListener('input', (event) => { ... });
            }

            // Initialize toggles based on saved preferences or defaults
            // (This part would typically involve localStorage and applying initial states)
            // For example:
            // if (settings.darkModeToggle) { settings.darkModeToggle.checked = localStorage.getItem('darkMode') === 'true'; }
            // Apply theme, contrast, etc., based on these initial values

            // Event listeners for toggles (dark mode, high contrast, etc.) are likely already set up elsewhere 
            // or would be added here if not.
        }


        document.addEventListener('DOMContentLoaded', () => {
            // Initialize GridWorldDemo first if it's needed by other UI initializations
            const initialSimSpeed = document.getElementById('simulationSpeed') ? parseInt(document.getElementById('simulationSpeed').value) : 50;
            const initialShowQ = document.getElementById('showQValues') ? document.getElementById('showQValues').checked : false;
            if (typeof GridWorldDemo !== 'undefined') {
                window.gridWorldDemo = new GridWorldDemo(initialShowQ, initialSimSpeed);
            }

            initializeSettingsUI(); // Initialize settings UI elements including value displays

            // Initialize other features like chat, tour, etc.
            if (typeof GuidedTour !== 'undefined') {
                window.guidedTour = new GuidedTour();
                // Optionally start tour if it's the first visit
                // if (!localStorage.getItem('tourCompleted')) {
                //     window.guidedTour.startTour();
                //     localStorage.setItem('tourCompleted', 'true');
                // }
            }

            const settingsPanel = document.getElementById('settingsPanel');
            const settingsBtn = document.getElementById('settingsBtn');
            const closeSettings = document.getElementById('closeSettings');

            if (settingsBtn) {
                settingsBtn.addEventListener('click', () => settingsPanel.classList.add('open'));
            }
            if (closeSettings) {
                closeSettings.addEventListener('click', () => settingsPanel.classList.remove('open'));
            }

            // Theme toggle logic (assuming it might be here or in its own module)
            const themeToggle = document.getElementById('themeToggle');
            const darkModeToggleSettings = document.getElementById('darkModeToggle'); // In settings panel
            const themeText = document.getElementById('themeText');
            const body = document.body;

            function applyTheme(isDark) {
                if (isDark) {
                    body.classList.add('dark-mode');
                    if (themeText) themeText.textContent = 'Light Mode';
                    if (themeToggle) themeToggle.setAttribute('aria-label', 'Toggle light mode');
                    if (darkModeToggleSettings) darkModeToggleSettings.checked = true;
                } else {
                    body.classList.remove('dark-mode');
                    if (themeText) themeText.textContent = 'Dark Mode';
                    if (themeToggle) themeToggle.setAttribute('aria-label', 'Toggle dark mode');
                    if (darkModeToggleSettings) darkModeToggleSettings.checked = false;
                }
            }

            const savedTheme = localStorage.getItem('theme');
            if (savedTheme === 'dark') {
                applyTheme(true);
            } else {
                applyTheme(false);
            }

            if (themeToggle) {
                themeToggle.addEventListener('click', () => {
                    const isDark = body.classList.toggle('dark-mode');
                    applyTheme(isDark);
                    localStorage.setItem('theme', isDark ? 'dark' : 'light');
                });
            }

            if (darkModeToggleSettings) {
                darkModeToggleSettings.addEventListener('change', (event) => {
                    const isDark = event.target.checked;
                    applyTheme(isDark);
                    localStorage.setItem('theme', isDark ? 'dark' : 'light');
                });
            }

            // Accessibility: High Contrast & Reduced Motion
            const highContrastToggle = document.getElementById('highContrast');
            const reducedMotionToggle = document.getElementById('reducedMotion');

            function applyHighContrast(isHighContrast) {
                if (isHighContrast) body.classList.add('high-contrast');
                else body.classList.remove('high-contrast');
            }
            function applyReducedMotion(isReducedMotion) {
                if (isReducedMotion) body.classList.add('reduced-motion');
                else body.classList.remove('reduced-motion');
            }

            if (highContrastToggle) {
                highContrastToggle.checked = localStorage.getItem('highContrast') === 'true';
                applyHighContrast(highContrastToggle.checked);
                highContrastToggle.addEventListener('change', (e) => {
                    applyHighContrast(e.target.checked);
                    localStorage.setItem('highContrast', e.target.checked);
                });
            }
            if (reducedMotionToggle) {
                reducedMotionToggle.checked = localStorage.getItem('reducedMotion') === 'true';
                applyReducedMotion(reducedMotionToggle.checked);
                reducedMotionToggle.addEventListener('change', (e) => {
                    applyReducedMotion(e.target.checked);
                    localStorage.setItem('reducedMotion', e.target.checked);
                });
            }

            // Font size adjustment from settings
            const fontSizeSlider = document.getElementById('fontSize');
            if (fontSizeSlider) {
                const initialFontSize = localStorage.getItem('fontSize') || '16';
                document.documentElement.style.fontSize = initialFontSize + 'px';
                fontSizeSlider.value = initialFontSize;
                // Add this if a display for font size value is also desired:
                // const fontSizeValueDisplay = document.getElementById('fontSizeValueDisplay'); 
                // if(fontSizeValueDisplay) fontSizeValueDisplay.textContent = initialFontSize + 'px';

                fontSizeSlider.addEventListener('input', (e) => {
                    document.documentElement.style.fontSize = e.target.value + 'px';
                    localStorage.setItem('fontSize', e.target.value);
                    // if(fontSizeValueDisplay) fontSizeValueDisplay.textContent = e.target.value + 'px';
                });
            }

            // Content Display Toggles
            const showDefinitionsToggle = document.getElementById('showDefinitions');
            if (showDefinitionsToggle) {
                // Logic to show/hide definitions based on this toggle
                showDefinitionsToggle.checked = localStorage.getItem('showDefinitions') !== 'false'; // default to true
                // Add event listener and function to toggle definition visibility globally
                // Example: body.classList.toggle('hide-definitions', !showDefinitionsToggle.checked);
            }
            const showAdvancedToggle = document.getElementById('showAdvanced');
            if (showAdvancedToggle) {
                // Logic to show/hide advanced content
                showAdvancedToggle.checked = localStorage.getItem('showAdvanced') === 'true';
                // Example: body.classList.toggle('show-advanced-content', showAdvancedToggle.checked);
            }

            // Ensure other initializations like tab functionality, scrollspy, etc., are also called.
            // For example, if there's a function initTabs(), call it here.
            // initTabs(); 
            // initScrollSpy();

            // Keyboard shortcuts
            document.addEventListener('keydown', (e) => {
                if (document.activeElement && (document.activeElement.tagName === 'INPUT' || document.activeElement.tagName === 'TEXTAREA')) {
                    return; // Don't trigger shortcuts if typing in an input/textarea
                }
                switch (e.key.toLowerCase()) {
                    case '/':
                        e.preventDefault();
                        if (searchInput) searchInput.focus();
                        break;
                    case 's':
                        e.preventDefault();
                        if (settingsPanel && settingsBtn) settingsPanel.classList.toggle('open');
                        break;
                    case 'c':
                        e.preventDefault();
                        if (chatContainer && chatFab) chatContainer.classList.toggle('active');
                        if (chatContainer && chatContainer.classList.contains('active') && chatInput) chatInput.focus();
                        break;
                    case 'd':
                        e.preventDefault();
                        if (themeToggle) themeToggle.click();
                        break;
                    // Add more shortcuts as needed, e.g., for tour, next/prev section, etc.
                }
            });

            // Parameter Controls Functionality
            document.addEventListener('DOMContentLoaded', function () {
                // Parameter sliders and their value displays
                const learningRateSlider = document.getElementById('learningRate');
                const learningRateValue = document.getElementById('learningRateValue');

                const discountFactorSlider = document.getElementById('discountFactor');
                const discountFactorValue = document.getElementById('discountFactorValue');

                const explorationRateSlider = document.getElementById('explorationRate');
                const explorationRateValue = document.getElementById('explorationRateValue');

                const episodesSlider = document.getElementById('episodes');
                const episodesValue = document.getElementById('episodesValue');

                // Update value displays when sliders change
                learningRateSlider.addEventListener('input', function () {
                    learningRateValue.textContent = this.value;
                    animateValueChange(learningRateValue);
                    updateRLParameters();
                });

                discountFactorSlider.addEventListener('input', function () {
                    discountFactorValue.textContent = this.value;
                    animateValueChange(discountFactorValue);
                    updateRLParameters();
                });

                explorationRateSlider.addEventListener('input', function () {
                    explorationRateValue.textContent = this.value;
                    animateValueChange(explorationRateValue);
                    updateRLParameters();
                });

                episodesSlider.addEventListener('input', function () {
                    episodesValue.textContent = this.value;
                    animateValueChange(episodesValue);
                    updateRLParameters();
                });

                // Animation for value changes
                function animateValueChange(element) {
                    element.classList.remove('param-value-updated');
                    // Trigger reflow to restart animation
                    void element.offsetWidth;
                    element.classList.add('param-value-updated');
                }

                // Function to update RL parameters in the simulation
                function updateRLParameters() {
                    // This will be connected to the grid world RL algorithm
                    // It passes the current values to the learning algorithm
                    if (window.gridWorldAgent) {
                        window.gridWorldAgent.updateParameters({
                            learningRate: parseFloat(learningRateSlider.value),
                            discountFactor: parseFloat(discountFactorSlider.value),
                            explorationRate: parseFloat(explorationRateSlider.value),
                            episodes: parseInt(episodesSlider.value)
                        });
                    }
                }
            });

        });

        // Potentially other functions from app_features.js like tab handling, scrollspy, etc. follow here. 
    </script>
</body>

</html>